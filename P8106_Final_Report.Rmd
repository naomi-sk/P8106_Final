---
title: "P8106: Severe Flu Prediction Project"
author:
  - "Naomi Simon-Kumar"
  - ns3782
date: "05/10/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
fontsize: 11pt
linestretch: 1
---


```{r setup, include=FALSE}

# Ensures R code is suppressed
knitr::opts_chunk$set(
  echo = FALSE,         
  warning = FALSE,      
  message = FALSE,      
  results = 'hide'      
)

```

```{r}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(patchwork)
library(corrplot)
library(table1)
library(pROC)
library(MASS)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(jpeg)
library(visdat)
library(vip)
library(tidymodels)
library(knitr)
library(pdp)
library(e1071)

# Set seed for reproducibility
set.seed(299)

# Load dataset
flu_df <- read_csv("severe_flu.csv")

# Convert categorical variables to factors
flu_df <- flu_df %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")),
    race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, levels = c(0, 1), labels = c("No", "Yes")),
    hypertension = factor(hypertension, levels = c(0, 1), labels = c("No", "Yes")),
    severe_flu = factor(severe_flu, levels = c(0, 1), labels = c("No", "Yes"))
  )

# Drop ID
flu_df <- dplyr::select(flu_df, -id)

# Split data into testing and training
data_split <- initial_split(flu_df, prop = 0.8)
training_data <- training(data_split)
testing_data <- testing(data_split)

# Set preprocessing
preproc <- c("center", "scale", "zv")

# Set up cross validation control - with 5 repeats
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 5,
                     summaryFunction = defaultSummary,
                     classProbs = TRUE)

```

# 1. Introduction

## 1.1. Background and Study Objectives

This study aims to build prediction models for identifying factors associated with the incidence of severe flu within 6 months post-vaccination. The analysis focuses on a dataset of 1,000 participants to understand risk factors for severe flu in a population administered the flu vaccine. The primary objectives of this project are to: (1) evaluate whether advanced predictive modeling techniques like boosting and support vector machines (SVM) provide superior predictive performance compared to simpler models, (2) develop a predictive risk score that quantifies the probability of experiencing severe flu based on individual characteristics, and (3) identify key demographic and clinical factors that predict severe flu risk and assess how these factors influence this risk.

## 1.2. Data Source and Description

The `severe_flu` dataset used for analysis contains demographic and clinical information from 1,000 participants. Variables include age (in years), gender (1 = Male, 0 = Female), race/ethnicity (1 = White, 2 = Asian, 3 = Black, 4 = Hispanic), smoking status (0 = Never smoked, 1 = Former smoker, 2 = Current smoker), height (in centimeters), weight (in kilograms), BMI (Body Mass Index, calculated as weight in kg divided by height in meters squared), diabetes status (0 = No, 1 = Yes), hypertension status (0 = No, 1 = Yes), systolic blood pressure (SBP, in mmHg), and LDL cholesterol (in mg/dL). The outcome of interest is a binary variable severe_flu indicating whether a participant experienced severe flu within 6 months post-vaccination (0 = No, 1 = Yes).

# 2. Exploratory Analysis

Exploratory data analysis was undertaken to examine the structure of the `severe_flu` dataset, including assessment of data distributions, and relationships between variables. The dataset contains 1,000 observations with 12 variables after removing the ID column. Categorical variables (gender, race, smoking status, diabetes, hypertension, and severe flu outcome) were converted to factors with appropriate labels. The dataset was verified to be complete with no missing observations. Summary statistics were calculated for all demographic and clinical variables. Visualisations were produced to compare characteristics.

## 2.1. Summary Statistics

**Table 1. Summary statistics of demographic and clinical characteristics in the study population ** 
```{r results='asis'}

# Set seed for reproducibility
set.seed(299)

# Reference code: https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html 

label(flu_df$age) <- "Age (yrs)"
label(flu_df$gender) <- "Gender"
label(flu_df$race) <- "Race/Ethnicity"
label(flu_df$smoking) <- "Smoking Status"
label(flu_df$height) <- "Height (cm)"
label(flu_df$weight) <- "Weight (kg)"
label(flu_df$bmi) <- "Body Mass Index (weight/ (height)^2)"
label(flu_df$diabetes) <- "Diabetes"
label(flu_df$hypertension) <- "Hypertension"
label(flu_df$SBP) <- "Systolic Blood Pressure (mmHg)"
label(flu_df$LDL) <- "LDL cholesterol (mg/dL)"

table1= table1(~ gender + race + smoking + height + weight + bmi + diabetes + hypertension + SBP + LDL, data=flu_df)
knitr::kable(table1)

```

Table 1 summarises the demographic and clinical characteristics of the study population (N = 1000). The sample was balanced by gender (52.2% female, 47.8% male), with predominantly White participants (65.6%). Most participants were never smokers (58.4%), with 14.5% reporting diabetes and 46.4% identified as hypertensive. Notably, the mean BMI was 27.9 kg/m² (SD=2.76), indicating the average subject would be considered clinically overweight. Mean systolic blood pressure was 130 mmHg (SD=7.88), and mean LDL cholesterol was 110 mg/dL (SD=19.7).

## 2.2. Exploratory Plots

### 2.2.1. Histograms

```{r}

## Histograms for numeric predictors

h1 <- ggplot(training_data, aes(x = age)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Age Distribution") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h2 <- ggplot(training_data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("BMI") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h3 <- ggplot(training_data, aes(x = SBP)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("Systolic BP") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h4 <- ggplot(training_data, aes(x = LDL)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("LDL Cholesterol") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h5 <- ggplot(training_data, aes(x = height)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Height") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h6 <- ggplot(training_data, aes(x = weight)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Weight") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

# Combine plots
# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-y-axis-labels
combined_histogram_numeric <- (h1 + h2) / (h3 + h4) / (h5 + h6)
wrap_elements(combined_histogram_numeric) +
  labs(tag = "Count") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )

```

**Figure 1. Distribution of continuous predictors in the study dataset**

Figure 1 presents histogram plots for numeric predictors in the study dataset. Age, BMI, SBP, LDL, height and weight are approximately normally distributed, with minimal right skew for LDL.


### 2.2.2. Boxplots

```{r}

# Boxplots of numeric predictors
# Ref code: https://patchwork.data-imaginist.com/reference/plot_annotation.html
theme_no_xlab <- theme_bw() + theme(axis.title.x = element_blank())
bp1 <- ggplot(training_data, aes(x = severe_flu, y = age)) + geom_boxplot() + theme_no_xlab
bp2 <- ggplot(training_data, aes(x = severe_flu, y = bmi)) + geom_boxplot() + theme_no_xlab
bp3 <- ggplot(training_data, aes(x = severe_flu, y = SBP)) + geom_boxplot() + theme_no_xlab
bp4 <- ggplot(training_data, aes(x = severe_flu, y = LDL)) + geom_boxplot() + theme_no_xlab
bp5 <- ggplot(training_data, aes(x = severe_flu, y = height)) + geom_boxplot() + theme_no_xlab
bp6 <- ggplot(training_data, aes(x = severe_flu, y = weight)) + geom_boxplot() + theme_no_xlab

# Combine all plots
boxplot_numeric <- (bp1 + bp2) / (bp3 + bp4) / (bp5 + bp6) 

# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-x-axis-labels 
wrap_elements(boxplot_numeric) +
  labs(tag = "Severe Flu") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom"
  )

```

**Figure 2. Distribution of continuous predictors by severe flu status in the study dataset** 

From the boxplots of continuous predictors, age, SBP, LDL, and height distributions appear similar between severe flu and non-severe flu groups, with considerable middle quartile overlap (Figure 2). BMI and weight show slightly higher median values in the severe flu group, however, the substantial overlap in their distributions suggest these differences may not be statistically significant.

### 2.2.2. Barplots

```{r}

# Bar Plots of Categorical Predictors by Outcome

bar1 <- ggplot(training_data, aes(x = gender, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + labs(fill = "Severe Flu") + theme_bw()

bar2 <- ggplot(training_data, aes(x = race, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar3 <- ggplot(training_data, aes(x = smoking, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar4 <- ggplot(training_data, aes(x = diabetes, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar5 <- ggplot(training_data, aes(x = hypertension, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL) + theme_bw() + theme(legend.position = "none")

# Combine using patchwork
((bar1 + bar2) / (bar3 + bar4) / (bar5 + plot_spacer())) +
  plot_layout(guides = "collect")

```

**Figure 3. Proportion of severe flu outcomes across categorical variables in the study dataset** 

Among categorical predictors, there appears to be differences in severe flu rates (Figure 3). Hispanic subjects show a higher proportion of severe flu cases compared to other racial groups. Diabetic patients also appear to have a slightly higher severe flu rate than non-diabetic individuals. Similarly current smokers have a slightly higher proportion of severe flu cases compared to never and former smokers. Gender and hypertension status show minimal differences in severe flu proportions across categories.

## 2.3. Correlation Analysis

```{r}

# Set seed for reproducibility
set.seed(299)

## Correlation Plot

# Correlation plot for numeric variables
numeric_vars <- training_data %>% dplyr::select(age, height, weight, bmi, SBP, LDL)
corrplot(cor(numeric_vars), method = "circle") 

```

**Figure 4. Correlation matrix between continuous predictors in the study dataset**

The correlation plot shows several relationships among continuous predictors. Weight and BMI have a strong positive correlation ($r \approx 0.8$), which may indicate collinearity. This is expected as BMI is calculated directly from weight and height. Therefore, interpretation of model results, including identifying important predictors of severe flu, should consider that these variables overlap in what they measure. Age and SBP have a moderate positive correlation ($r \approx 0.4$). Height and weight are weakly positively correlated ($r \approx 0.2$). By comparison, height has a moderate negative correlation with BMI ($r \approx -0.4$).

# 3. Model Training

## 3.1. Data preprocessing

The data was examined to identify missing values and undertake data cleaning. No missing observations were identified. Categorical variables were converted into appropriate factor variables: gender (Female/Male), race/ethnicity (White, Asian, Black, Hispanic), smoking status (Never, Former, Current), diabetes (No/Yes), hypertension (No/Yes), and the response variable severe flu (No/Yes). ID was removed as it was not a meaningful predictor for flu severity. The dataset was then split into training (80%) and testing (20%) sets for model development and evaluation. All models except MARS and GAM were trained using centered and scaled data to ensure preprocessing was applied to models sensitive to feature scale.

## 3.2. Model Evaluation Approach

Kappa and classification accuracy were selected as the metrics for evaluating classification performance across all candidate models, with larger values representing better performance. Accuracy measures the overall proportion of correct classifications, while Kappa represents agreement between predicted and actual class labels occurring by chance. Kappa values range from 0 (no agreement beyond chance) to 1 (perfect agreement) (McHugh, 2012). These metrics were computed during cross-validation (10 fold with 5 repeats using the R `caret` package) on the training dataset for selecting optimal parameters. Final model performance was then evaluated on the held-out test dataset. This approach allowed for fair comparison between simple models (logistic regression, LDA) and more complex techniques (SVM, boosting) (Kuhn & Johnson, 2013). Confusion matrices were generated from the test dataset evaluation for the final selected model to report classification accuracy, sensitivity, and specificity at the 0.3 probability threshold. This threshold was selected in line with guidance from Saito and Rehmsmeier (2015), who note that threshold selection make careful consideration of imbalanced classification. The adjustment was chosen to prioritise sensitivity by reducing the likelihood of false negatives, which is relevant for our study.

## 3.3. Models

Eight predictive models for severe flu were evaluated: logistic regression, penalized logistic regression (elastic net), linear discriminant analysis (LDA), multivariate adaptive regression splines (MARS), generalized additive model (GAM), linear support vector machine (SVM), radial kernel SVM, and boosting (AdaBoost).

### 3.3.1 Simpler Models

#### Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)

# Train logistic regression model with CV
model.glm <- train(severe_flu ~ ., 
                   data = training_data,
                   method = "glm",
                   preProcess = preproc,
                   trControl = ctrl)

```

#### Penalized Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)


# Define tuning grid for penalized logistic regression
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-8, 0, length = 50)))

# Fit model with CV
model.glmn <- train(severe_flu ~ .,
                    data = training_data,
                    preProcess = preproc,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    trControl = ctrl)

# Show best tuning parameters
model.glmn$bestTune

# Plot the results
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))


# Coefficients in the final model
coef(model.glmn$finalModel, model.glmn$bestTune$lambda)

```

#### LDA

```{r}

# Set seed for reproducibility
set.seed(299)


# Fit the LDA model using MASS package
lda.fit <- lda(severe_flu ~ ., data = training_data)

# Plot the histogram of discriminant scores (Z-variable, Z = a^T*X)
plot(lda.fit)

# Fit LDA model using caret
model.lda <- train(severe_flu ~ .,
                   data = training_data,
                   preProcess = preproc,
                   method = "lda",
                   trControl = ctrl)

```

### 3.3.2 Moderately Complex Models

#### MARS


```{r}

# Set seed for reproducibility
set.seed(299)

# Tuning grid for degree and number of terms (nprune)
mars_grid <- expand.grid(
  degree = 1:4, # degree of interactions
  nprune = 2:20 # no. of retained terms
)

# Fit the MARS model
model.mars <- train(severe_flu ~ .,
                    data = training_data,
                    method = "earth",
                    tuneGrid = mars_grid,
                    trControl = ctrl)

# Plot CV results
ggplot(model.mars)

# Best parameters
model.mars$bestTune

# Coefficients for final model
coef(model.mars$finalModel)

# Partial dependence plot for 'bmi'
pdp::partial(model.mars, pred.var = c("bmi"), grid.resolution = 200) |> autoplot()

# Partial dependence plot for 'LDL'
pdp::partial(model.mars, pred.var = c("LDL"), grid.resolution = 200) |> autoplot()


```

The MARS model can be represented as follows:

$$
\begin{aligned} 
\log(\text{odds of severe flu})= -2.26535278 + 0.35193909 × h(bmi-26.6) + 0.01153187 × h(LDL-77)
\end{aligned}
$$

where $h(x) = \max(0,x)$ is the hinge function.


```{r}

# Variable importance plot for MARS
vip(model.mars$finalModel, type = "nsubsets")

```

#### GAM

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit GAM model using caret
model.gam <- train(
  severe_flu ~ .,
  data = training_data,
  method = "gam",
  trControl = ctrl
)

# Summary
summary(model.gam)

# Examine final model parameters
model.gam$finalModel

# Plot smooth GAM terms
# Code source: https://bookdown.org/ndphillips/YaRrr/arranging-plots-with-parmfrow-and-layout.html
par(mfrow = c(3, 2))  
plot(model.gam$finalModel)
par(mfrow = c(1, 1)) # Reset plotting window

```

### 3.3.3 Complex Models

#### Support Vector Machine Linear


```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model using SVM (linear)
svml.fit <- train(severe_flu ~ . ,
                  data = training_data,
                  preProcess = preproc,
                  method = "svmLinear",
                  tuneGrid = data.frame(C = exp(seq(-6, 2, len = 50))),
                  trControl = ctrl)

# Plot
plot(svml.fit, highlight = TRUE, xTrans = log)

# Find optimal
svml.fit$bestTune # optimal C = 0.004762708

```

#### Support Vector Machine Radial Kernel


```{r}

# Set seed for reproducibility
set.seed(299)

# Define tuning grid for Radial Kernel
svmr.grid <- expand.grid(C = exp(seq(-1, 5, len = 10)),
                         sigma = exp(seq(-12, -2, len = 10)))

# Fit radial kernel model
svmr.fit <- train(severe_flu ~ . , 
                  data = training_data,
                  method = "svmRadialSigma",
                  preProcess = preproc,
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

# Best sigma and cost
svmr.fit$bestTune # sigma = 0.00482795

# Plot
plot(svmr.fit, highlight = TRUE, xTrans = log)

```

#### Boosting

# 4. Results

## 4.1. Model Comparison and Selection

Repeated 10-fold cross-validation (with 5 repeats) was implemented to objectively compare performance across the models, using Kappa as the primary evaluation metric, with Accuracy as a secondary evaluation metric. This cross-validation approach allowed for robust comparison while minimising the risk of overfitting. The final model was then assessed on the held-out test set (20% of the original data) to validate performance on unseen data.

```{r}

# Set seed for reproducibility
set.seed(299)

# Use cross validation to perform model selection
res <- resamples(list(Logistic_Regression = model.glm,
                      Penalised_Logistic_Regression = model.glmn,
                      LDA = model.lda,
                      MARS = model.mars,
                      GAM = model.gam,
                      SVM_Linear = svml.fit,
                      SVM_Radial_Kernel = svmr.fit,
                      AdaBoost = gbmA.fit))

bwplot(res)

```

**Figure. Boxplot of cross-validated Accuracy and Kappa values for all candidate models.**

```{r}
# Set seed for reproducibility
set.seed(299)

# Get mean Accuracy and Kappa from resamples
res_summary <- summary(res)

# Get Accuracy and Kappa means
acc <- res_summary$statistics$Accuracy[, "Mean"]
kap <- res_summary$statistics$Kappa[, "Mean"]

# Table
kable(data.frame(
  Accuracy = round(acc, 4),
  Kappa = round(kap, 4)
))

```

**Table 1. Cross-validated Accuracy and Kappa values (mean) for candidate models.**

Table 1 shows that the SVM radial kernel model had the highest mean accuracy (0.7755), however logistic regression had a very similar performance (0.7750). LDA (0.2214) and GAM (0.2200) had the highest Kappa values. Logistic regression also had a similar Kappa value (0.2152). By comparison, both SVM models had comparably lower Kappa values. Kappa was emphasised as the primary evaluation metric, given the class imbalance identified in the data and its ability to adjust for agreement due to chance. Therefore, given its strong performance across both metrics, logistic regression was selected as the final model.

## 4.2. Preferred Model for Clinical Use: Logistic Regression

Although SVM radial kernel was slightly better performing in terms of accuracy compared to logistic regression, as a black-box model, it lacks interpretability in the context of the study, particularly as it does not provide direct risk probabilities. In comparison, logistic regression had a comparable accuracy and one of the highest Kappa values. Importantly, it also has the advantage of offering clear interpretation through coefficients and predicted probabilities compared to other models, including black box SVM models. Therefore, logistic regression is preferrable in this context for the development of clinically relevant and interpretable risk scores. As an important consideration, scholars including Kerr et al. (2014) caution that very small improvements in discrimination metrics may lack clinical meaning and may not justify the added complexity of new models, especially when they do not translate into improvements in patient outcomes or clinical decision-making. This is further supported by a systematic review of 71 clinical prediction studies which found no performance advantage of complex machine learning methods over traditional logistic regression for clinical use (Christodoulou et al., 2019).

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict probabilities on test data
glm.pred <- predict(model.glm, 
                    newdata = testing_data, 
                    type = "prob")[, "Yes"]

# Convert probabilities to class predictions, using 0.3 cutoff
glm.class <- rep("No", length(glm.pred))
glm.class[glm.pred > 0.3] <- "Yes"

# Confusion matrix
confusionMatrix(
  data = factor(glm.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# Compute ROC 
roc.glm <- roc(testing_data$severe_flu, glm.pred)

```

A 0.3 probability threshold was used to assign predicted classes in evaluation of the model based on test data. The accuracy of the logistic regression model is **0.695**, or 69.5%. The misclassification rate is **0.305**, meaning about 30.5% of test observations were incorrectly classified. The P-Value [Acc > NIR] is **0.7098**, which indicates that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not outperform a naive classifier that always predicts the majority class. The model predictions indicate 112 true negatives (non-severe flu cases correctly classified as “No”), and 27 true positives (severe flu cases correctly classified as “Yes”). The model gave 30 false positives (non-severe flu cases misclassified as “Yes”) and 31 false negatives (severe flu cases misclassified as “No”). Sensitivity was **0.4655**, which shows that the model correctly identified 46.55% of severe flu cases. By comparison, specificity was **0.7887**, indicating that 78.87% of non-severe cases were correctly classified. The Kappa statistic was **0.2556**, indicating fair agreement between predicted and actual class labels, beyond what would be expected by chance. 

```{r}

# Set seed for reproducibility
set.seed(299)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

```
**Figure 6. ROC curve for the logistic regression model based on the test dataset, with an AUC of 0.703**

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.703**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds.

## 4.4. Predictive Risk Score

Addressing the second study objective, predicted probabilities (predicted risk scores) were calculated based on the logistic regression model to quantify the likelihood of experiencing severe flu, based on individual participant characteristics.

```{r}

# Set seed for reproducibility
set.seed(299)

# Distribution of predicted risk scores from logistic regression model
risk_scores <- glm.pred

# predicted probabilities and severe flu outcomes
risk_df <- data.frame(
  risk_score = risk_scores,
  severe_flu = testing_data$severe_flu
)

# Plot distribution of risk scores
ggplot(risk_df, aes(x = risk_score, fill = severe_flu)) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  scale_fill_manual(values = c("lightblue", "pink")) +
  labs(x = "Predicted Probability (Risk Score)", 
       y = "Count",
       title = "Distribution of Severe Flu Risk Scores") 

```

**Figure 7. Distribution of severe flu risk scores by actual severe flu outcome. The histogram shows the distribution of predicted probabilities from the logistic regression model by outcome (blue = no severe flu, pink = severe flu).**

From the histogram plot, it is observed that there is substantial overlap between subjects who experienced severe flu and those who did not. Although patients who developed severe flu tend to be higher in the upper ranges of predicted probabilities (above 0.4) and slightly lower representation in the lowest risk range (0.1-0.2), many still received relatively low risk scores (under 0.3). The overlap in distributions for both severe and non-severe flu groups suggests the model performs relatively well in assigning higher risk to true cases, but it does not distinguish the two groups with near perfect reliability. This likely reflects the model's modest discriminative ability as well as the class imbalance in the dataset. The predicted risk of experiencing severe flu can be quantified using the fitted logistic regression model, which estimates the log-odds of severe flu as a linear combination of demographic and clinical predictors. The model equation is:

$$
\begin{aligned}
\log(\text{odds of severe flu}) =\ 
& - 1.29276916 -0.12451889 \cdot \text{age} + 0.15509135 \cdot \text{gender}_{\text{Male}} \\
& +\ 0.07968378 \cdot \text{race}_{\text{Asian}} - 0.06840200 \cdot \text{race}_{\text{Black}} + 0.12722560 \cdot \text{race}_{\text{Hispanic}} \\
& -\ 0.09420215 \cdot \text{smoking}_{\text{Former}} + 0.18344992 \cdot \text{smoking}_{\text{Current}} \\
& +\ 2.58962594 \cdot \text{height} - 3.23060603 \cdot \text{weight} + 4.24941691 \cdot \text{BMI} \\
& +\ 0.17126025 \cdot \text{diabetes}_{\text{Yes}} + 0.22275667 \cdot \text{hypertension}_{\text{Yes}} \\
& -\ 0.08938464 \cdot \text{SBP} + 0.22156244 \cdot \text{LDL}
\end{aligned}
$$

Note that the predicted probability for severe flu based on individual participant characteristics can be calculated as:

$$
P(\text{Severe Flu} = 1 \mid X) = \frac{e^{\text{log-odds}}}{1 + e^{\text{log-odds}}}
$$

where log-odds is the linear predictor estimated by the logistic regression model above.


### 4.5. Key demographic and clinical predictors of Severe Flu 

```{r}

# Set seed for reproducibility
set.seed(299)

# Get coefficients from logistic regression model
coef_table <- cbind(
  Coefficient = coef(model.glm$finalModel),
  OddsRatio = exp(coef(model.glm$finalModel))
)

# Format and display the table
kable(coef_table, digits = 3)

```
**Table 3. Logistic Regression model coefficients and odds ratios for severe flu prediction**

Table 3 shows the logistic regression model coefficients and odds ratios for severe flu prediction. BMI showed the strongest positive association with severe flu risk (OR = 70.065). This may potentially be due to multicollinearity with height and weight (OR = 13.325 and OR = 0.040, respectively), rather than a true independent effect. Therefore, these individual odds ratios should be interpreted with caution, though overall risk score validity remains strong. In terms of clinical relevance, this would suggest that body composition is associated with severe flu, though the specific contribution of each measure cannot be concluded. Several demographic factors, including being male (OR = 1.168) and Hispanic race (OR = 1.136) were associated with slightly higher odds of severe flu compared to females and White individuals respectively. Clinical factors including hypertension (OR = 1.250), diabetes (OR = 1.187), and LDL cholesterol (OR = 1.248) were also associated with an increase in odds of severe flu. Current smoking was associated with an approximate 20% higher odds of severe flu (OR = 1.201) compared to never smoking. However, each additional year of age was associated with approximately 12% lower odds of severe flu (OR = 0.883), which may reflect confounding or cohort effects warranting further inquiry.

```{r}

# coefficient plot 
coef_plot_data <- coef_df[-1, ] 
coef_plot_data$Predictor <- factor(coef_plot_data$Predictor)
coef_plot_data$Direction <- ifelse(coef_plot_data$Coefficient > 0, "Positive", "Negative")

ggplot(coef_plot_data, aes(x = reorder(Predictor, Coefficient), y = Coefficient, fill = Direction)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("Negative" = "lightblue", "Positive" = "pink")) +
  labs(x = "Predictor", y = "Coefficient (Log Odds)") 

```

**Figure 7. Magnitude and direction of logistic regression model coefficients on log-odds of experiencing severe flu**

# 5. Limitations

A significant limitation of this study is the class imbalance in the dataset, which most likely contributed to low sensitivity across models. It is important to note that while some models showed relatively high classification accuracy, their low Kappa values suggest that performance may be attributable to majority class prediction, rather than good agreement. Additionally, black-box models offer limited interpretability, which makes them less practical in this specific clinical context. 

# 6. Conclusion

In summary, although the SVM with radial kernel had the highest mean cross-validated accuracy, logistic regression demonstrated nearly identical performance, with stronger interpretability and higher Kappa. In fact, the kappa for both SVM models did not indicate substantial improvement in agreement beyond chance, which suggests practical limitations. Therefore, while advanced models may offer some minor improvements in predictive accuracy, they may not be justified in this clinical setting. Standard logistic regression is strongly recommended over more complex models. Evaluation of the final logistic regression model showed a test accuracy of 69.5%, with a misclassification rate of 30.5%. However, the model did not significantly outperform the no information rate (NIR = 0.71; p = 0.7098). Sensitivity was in the moderate range (46.55%), which indicates that further optimisation of the classification threshold may be necessary depending on clinical priorities. Future research should address the class imbalance identified in the dataset and optimise classification thresholds to improve model performance in detecting severe flu cases in the population of interest.

# References

McHugh, M. L. (2012). Interrater reliability: the kappa statistic. Biochemia medica, 22(3), 276-282.

Christodoulou, E., Ma, J., Collins, G. S., Steyerberg, E. W., Verbakel, J. Y., & Van Calster, B. (2019). A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. Journal of clinical epidemiology, 110, 12-22.

Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). Applied logistic regression (3rd ed.). John Wiley & Sons.

Kerr, K. F., Wang, Z., Janes, H., McClelland, R. L., Psaty, B. M., & Pepe, M. S. (2014). Net reclassification indices for evaluating risk prediction instruments: A critical review. Epidemiology, 25(1), 114–121. https://doi.org/10.1097/EDE.0000000000000018:contentReference[oaicite:3]{index=3}

Kuhn, M., & Johnson, K. (2013). Applied predictive modeling. Springer.