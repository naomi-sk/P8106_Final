---
title: "P8106: Severe Flu Prediction Project"
author:
  - "Naomi Simon-Kumar"
  - ns3782
date: "05/10/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
fontsize: 11pt
linestretch: 1
---


```{r setup, include=FALSE}

# Ensures R code is suppressed
knitr::opts_chunk$set(
  echo = FALSE,         
  warning = FALSE,      
  message = FALSE,      
  results = 'hide'      
)

```

```{r}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(patchwork)
library(corrplot)
library(table1)
library(pROC)
library(MASS)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(jpeg)
library(visdat)
library(vip)
library(tidymodels)
library(knitr)

# Set seed for reproducibility
set.seed(299)

# Load dataset
flu_df <- read_csv("severe_flu.csv")

# Convert categorical variables to factors
flu_df <- flu_df %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")),
    race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, levels = c(0, 1), labels = c("No", "Yes")),
    hypertension = factor(hypertension, levels = c(0, 1), labels = c("No", "Yes")),
    severe_flu = factor(severe_flu, levels = c(0, 1), labels = c("No", "Yes"))
  )

# Drop ID
flu_df <- dplyr::select(flu_df, -id)

# Set 10-fold cross-validation
ctrl1 <- trainControl(method = "cv", number = 10)

# Split data into testing and training
data_split <- initial_split(flu_df, prop = 0.8)
training_data <- training(data_split)
testing_data <- testing(data_split)

```

# 1. Introduction

## 1.1. Background and Study Objectives

This study aims to build prediction models for identifying factors associated with the incidence of severe flu within 6 months post-vaccination. The analysis focuses on a dataset of 1,000 participants to understand risk factors for severe flu in a population administered the flu vaccine. The primary objectives of this project are to: (1) evaluate whether advanced predictive modeling techniques like boosting and support vector machines (SVM) provide superior predictive performance compared to simpler models, (2) develop a predictive risk score that quantifies the probability of experiencing severe flu based on individual characteristics, and (3) identify key demographic and clinical factors that predict severe flu risk and assess how these factors influence this risk.

## 1.2. Data Source and Description

The `severe_flu` dataset used for analysis contains demographic and clinical information from 1,000 participants. Variables include age (in years), gender (1 = Male, 0 = Female), race/ethnicity (1 = White, 2 = Asian, 3 = Black, 4 = Hispanic), smoking status (0 = Never smoked, 1 = Former smoker, 2 = Current smoker), height (in centimeters), weight (in kilograms), BMI (Body Mass Index, calculated as weight in kg divided by height in meters squared), diabetes status (0 = No, 1 = Yes), hypertension status (0 = No, 1 = Yes), systolic blood pressure (SBP, in mmHg), and LDL cholesterol (in mg/dL). The outcome of interest is a binary variable severe_flu indicating whether a participant experienced severe flu within 6 months post-vaccination (0 = No, 1 = Yes).

# 2. Exploratory Analysis

Exploratory data analysis was undertaken to examine the structure of the `severe_flu` dataset, including assessment of data distributions, and relationships between variables. The dataset contains 1,000 observations with 12 variables after removing the ID column. Categorical variables (gender, race, smoking status, diabetes, hypertension, and severe flu outcome) were converted to factors with appropriate labels. The dataset was verified to be complete with no missing observations. Summary statistics were calculated for all demographic and clinical variables. Visualisations were produced to compare characteristics between participants who did and did not experience severe flu.

## 2.1. Summary Statistics

**Table 1. Summary statistics of demographic and clinical characteristics in the study population ** 
```{r results='asis'}

# Set seed for reproducibility
set.seed(299)

# Reference code: https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html 

label(flu_df$age) <- "Age (yrs)"
label(flu_df$gender) <- "Gender"
label(flu_df$race) <- "Race/Ethnicity"
label(flu_df$smoking) <- "Smoking Status"
label(flu_df$height) <- "Height (cm)"
label(flu_df$weight) <- "Weight (kg)"
label(flu_df$bmi) <- "Body Mass Index (weight/ (height)^2)"
label(flu_df$diabetes) <- "Diabetes"
label(flu_df$hypertension) <- "Hypertension"
label(flu_df$SBP) <- "Systolic Blood Pressure (mmHg)"
label(flu_df$LDL) <- "LDL cholesterol (mg/dL)"

table1= table1(~ gender + race + smoking + height + weight + bmi + diabetes + hypertension + SBP + LDL, data=flu_df)
knitr::kable(table1)

```

Table 1 summarises the demographic and clinical characteristics of the study population (N = 1000). The sample was balanced by gender (52.2% female, 47.8% male), with predominantly White participants (65.6%). Most participants were never smokers (58.4%), with 14.5% reporting diabetes and 46.4% identified as hypertensive. Notably, the mean BMI was 27.9 kg/mÂ² (SD=2.76), indicating the average subject would be considered clinically overweight. Mean systolic blood pressure was 130 mmHg (SD=7.88), and mean LDL cholesterol was 110 mg/dL (SD=19.7).

## 2.2. Exploratory Plots

### 2.2.1. Histograms

```{r}

## Histograms for numeric predictors

h1 <- ggplot(training_data, aes(x = age)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Age Distribution") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h2 <- ggplot(training_data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("BMI") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h3 <- ggplot(training_data, aes(x = SBP)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("Systolic BP") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h4 <- ggplot(training_data, aes(x = LDL)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("LDL Cholesterol") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h5 <- ggplot(training_data, aes(x = height)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Height") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h6 <- ggplot(training_data, aes(x = weight)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Weight") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

# Combine plots
# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-y-axis-labels
combined_histogram_numeric <- (h1 + h2) / (h3 + h4) / (h5 + h6)
wrap_elements(combined_histogram_numeric) +
  labs(tag = "Count") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )

```

**Figure 1. Distribution of continuous predictors in the study dataset**

Figure 1 presents histogram plots for numeric predictors in the study dataset. Age, BMI, SBP, LDL, height and weight are approximately normally distributed, with minimal right skew for LDL.


### 2.2.2. Boxplots

```{r}

# Boxplots of numeric predictors
# Ref code: https://patchwork.data-imaginist.com/reference/plot_annotation.html
theme_no_xlab <- theme_bw() + theme(axis.title.x = element_blank())
bp1 <- ggplot(training_data, aes(x = severe_flu, y = age)) + geom_boxplot() + theme_no_xlab
bp2 <- ggplot(training_data, aes(x = severe_flu, y = bmi)) + geom_boxplot() + theme_no_xlab
bp3 <- ggplot(training_data, aes(x = severe_flu, y = SBP)) + geom_boxplot() + theme_no_xlab
bp4 <- ggplot(training_data, aes(x = severe_flu, y = LDL)) + geom_boxplot() + theme_no_xlab
bp5 <- ggplot(training_data, aes(x = severe_flu, y = height)) + geom_boxplot() + theme_no_xlab
bp6 <- ggplot(training_data, aes(x = severe_flu, y = weight)) + geom_boxplot() + theme_no_xlab

# Combine all plots
boxplot_numeric <- (bp1 + bp2) / (bp3 + bp4) / (bp5 + bp6) 

# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-x-axis-labels 
wrap_elements(boxplot_numeric) +
  labs(tag = "Severe Flu") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom"
  )

```

**Figure 2. Distribution of continuous predictors by severe flu status in the study dataset** 

From the boxplots of continuous predictors, age, SBP, LDL, and height distributions appear similar between severe flu and non-severe flu groups, with considerable middle quartile overlap (Figure 2). BMI and weight show slightly higher median values in the severe flu group, however, the substantial overlap in their distributions suggest these differences may not be statistically significant.

### 2.2.2. Barplots

```{r}

# Bar Plots of Categorical Predictors by Outcome

bar1 <- ggplot(training_data, aes(x = gender, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + labs(fill = "Severe Flu") + theme_bw()

bar2 <- ggplot(training_data, aes(x = race, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar3 <- ggplot(training_data, aes(x = smoking, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar4 <- ggplot(training_data, aes(x = diabetes, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar5 <- ggplot(training_data, aes(x = hypertension, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL) + theme_bw() + theme(legend.position = "none")

# Combine using patchwork
((bar1 + bar2) / (bar3 + bar4) / (bar5 + plot_spacer())) +
  plot_layout(guides = "collect")

```

**Figure 3. Proportion of severe flu outcomes across categorical variables in the study dataset** 

Among categorical predictors, there appears to be differences in severe flu rates (Figure 3). Hispanic subjects show a higher proportion of severe flu cases compared to other racial groups. Diabetic patients also appear to have a slightly higher severe flu rate than non-diabetic individuals. Similarly current smokers have a slightly higher proportion of severe flu cases compared to never and former smokers. Gender and hypertension status show minimal differences in severe flu proportions across categories.

## 2.3. Correlation Analysis


```{r}

# Set seed for reproducibility
set.seed(299)

## Correlation Plot

# Correlation plot for numeric variables
numeric_vars <- training_data %>% dplyr::select(age, height, weight, bmi, SBP, LDL)
corrplot(cor(numeric_vars), method = "circle") 

```

**Figure 4. Correlation matrix between continuous predictors in the study dataset**

The correlation plot shows several relationships among continuous predictors. Weight and BMI have a strong positive correlation ($r \approx 0.8$), which may indicate collinearity. This is expected as BMI is calculated directly from weight and height. Therefore, interpretation of model results, including identifying important predictors of severe flu, should consider that these variables overlap in what they measure. Age and SBP have a moderate positive correlation ($r \approx 0.4$). Height and weight are weakly positively correlated ($r \approx 0.2$). By comparison, height has a moderate negative correlation with BMI ($r \approx -0.4$).

# 3. Model Training

## 3.1. Data preprocessing

The data was examined to identify missing values and undertake data cleaning. No missing observations were identified. Categorical variables were converted into appropriate factor variables: gender (Female/Male), race/ethnicity (White, Asian, Black, Hispanic), smoking status (Never, Former, Current), diabetes (No/Yes), hypertension (No/Yes), and the response variable severe flu (No/Yes). The ID variable was removed as it was not a meaningful predictor for flu severity. The preprocessed dataset was then split into training (80%) and testing (20%) sets for model development and evaluation.

## 3.2. Model Evaluation metrics

Area Under the Receiver Operating Characteristic curve (AUC) was selected as the metric for evaluating classification performance across all candidate models. AUC measures a model's ability to discriminate between severe and non-severe flu cases across all possible classification thresholds, with values ranging from 0.5 (no better than random chance) to 1.0 (perfect discrimination) (Hosmer et al., 2013). AUC was computed during cross-validation (10-fold with 5 repeats) on the training dataset to select optimal model parameters. Final model performance was then evaluated on the held-out test dataset. This approach allowed for fair comparison between simple models (logistic regression, LDA) and more complex techniques (SVM, boosting) while being robust to class imbalance in the dataset (Kuhn & Johnson, 2013). Confusion matrices were generated for the final selected model to report classification accuracy, sensitivity, and specificity at the 0.5 probability threshold, following the Bayes decision rule, which assigns an observation to the most likely class. This was undertaken to assess predictive performance after model selection based on cross-validated AUC.

## 3.3. Models


# 4. Results

## 4.1. Model Comparison and Selection

Eight predictive models for severe flu were evaluated: logistic regression, penalized logistic regression (elastic net), linear discriminant analysis (LDA), multivariate adaptive regression splines (MARS), generalized additive model (GAM), linear support vector machine (SVM), radial kernel SVM, and adaptive boosting (AdaBoost). Repeated 10-fold cross-validation (with 5 repeats) was implemented to objectively compare performance across the models, using AUC as the primary evaluation metric. This cross-validation approach allowed for robust comparison while minimizing the risk of overfitting. Models were then assessed on the held-out test set (20% of the original data) to validate performance and ensure the selected model would generalise appropriately to new observations from the same population.

```{r}

# Set seed for reproducibility
set.seed(299)

# Use cross validation to perform model selection
res <- resamples(list(Logistic_Regression = model.glm,
                      Penalised_Logistic_Regression = model.glmn,
                      LDA = model.lda,
                      MARS = model.mars,
                      GAM = model.gam,
                      SVM_Linear = svml.fit,
                      SVM_Radial_Kernel = svmr.fit,
                      AdaBoost = gbmA.fit))
summary(res)

bwplot(res, metric="ROC")

```
**Figure. Boxplot of cross-validated AUC (ROC) values for all candidate models. SVM with a radial kernel achieved the highest average AUC and showed consistent performance across resamples, while other models such as penalized logistic regression and logistic regression also performed comparably well.**

```{r}

# Set seed for reproducibility
set.seed(299)

# Extract mean AUC values 
roc_df <- as_tibble(summary(res)$statistics$ROC, rownames = "Model")

# Select only Model and AUC
roc_clean <- roc_df %>%
  transmute(Model, AUC = round(Mean, 3))

# Display table
kable(roc_clean)

```
**Table 2. Mean cross-validated AUC (ROC) values for all candidate models. SVM with a radial kernel achieved the highest average AUC (0.701).**

The cross-validation results show that the SVM with a radial kernel achieved the highest mean AUC (0.701). Penalized logistic regression (AUC = 0.696) and standard logistic regression (AUC = 0.695) had a similar performance.

## 4.2. Best performing model evaluation

Based on cross-validation results, the SVM with radial kernel (SVM-radial) was selected as the best performing model, achieving the highest mean AUC (0.701) when implemented. The model was evaluated on the held-out test set to assess its generalizability to new data.

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict probabilities on test data using radial SVM
svm.radial.pred <- predict(svmr.fit, newdata = testing_data, type = "prob")[, "Yes"]

# Convert to class predictions using 0.5 cutoff
svm.radial.class <- ifelse(svm.radial.pred > 0.5, "Yes", "No")

# Confusion Matrix
confusionMatrix(factor(svm.radial.class, levels = c("No", "Yes")),
                testing_data$severe_flu,
                positive = "Yes")

# ROC and AUC
roc.svm.radial <- roc(testing_data$severe_flu, svm.radial.pred)
plot(roc.svm.radial, legacy.axes = TRUE, print.auc = TRUE)

```
**Figure 5. ROC curve for the SVM-radial model on the test dataset, showing an AUC of 0.699**

The SVM radial kernel model achieved an AUC of 0.699 when fitted on the test dataset, indicating moderate discriminative ability between severe and non-severe flu cases. This suggests the model generalises well to unseen data without overfitting. Based on predictions on the test dataset, the accuracy of the radial kernel SVM model is **0.745**, or 74.5%. The misclassification rate is **0.255**, meaning 25.5% of test observations were incorrectly classified. The **P-Value [Acc > NIR] is 0.1554**, indicating that the modelâs accuracy is not significantly better than the no information rate (NIR = 0.71). This would indicate that the model does not outperform a naive classifier that always predicts the majority class, non-severe flu. Based on test data predictions, the model gave 139 true negatives (non-severe flu cases correctly classified as âNoâ) and 10 true positives (severe flu cases correctly classified as âYesâ). There were 48 false negatives (severe flu cases misclassified as âNoâ) and 3 false positives (non-severe cases misclassified as âYesâ). The model showed high specificity (97.89%), accurately identifying individuals who would not develop severe flu. However, its sensitivity was comparatively low (17.24%), indicating limited ability to correctly identify those who would develop severe flu. The Kappa statistic is **0.1963**, representing weak agreement between predicted and actual class labels, only slightly better than chance. These results suggest that while advanced modelling approaches like SVM offer some improvement in predictive performance, the gain may not be practically significant when compared to simpler approaches like logistic regression.

## 4.3. Preferred Model for Clinical Use: Logistic Regression

While SVM radial kernel was slightly better performing compared to logistic regression, as a black-box model, it lacks interpretability in the context of the study, particularly as it does not provide direct risk probabilities. In comparison, logistic regression was comparable in predictive performance (AUC = 0.695), with a sensitivity of 0.1897 and specificity of 0.9577. It also has the advantage of offering interpretation through coefficients and predicted probabilities. Therefore, while the SVM radial kernel model may be slightly better for prediction accuracy, logistic regression is preferred in this clinical context involving risk assessment for individual subjects, consistent with the modelling tradeoffs emphasised in data science practice. As an important consideration, scholars including Kerr et al. (2014) caution that very small improvements in discrimination metrics such as AUC may lack clinical meaning and may not justify the added complexity of new models, especially when they do not translate into improvements in patient outcomes or clinical decision-making. This is further supported by a systematic review of 71 clinical prediction studies which found no performance advantage of complex machine learning methods over traditional logistic regression for clinical use (Christodoulou et al., 2019).

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict probabilities on test data
glm.pred <- predict(model.glm, 
                    newdata = testing_data, 
                    type = "prob")[, "Yes"]

# Convert probabilities to class predictions, using 0.5 cutoff
glm.class <- rep("No", length(glm.pred))
glm.class[glm.pred > 0.5] <- "Yes"

# Confusion matrix
confusionMatrix(
  data = factor(glm.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# Compute ROC 
roc.glm <- roc(testing_data$severe_flu, glm.pred)

```

Based on predictions on the test dataset, the accuracy of the logistic regression model is **0.735**, or 73.5%. The misclassification rate is **0.265**, meaning about 26.5% of test observations were incorrectly classified. The P-Value [Acc > NIR] is **0.2434**, which indicates that the modelâs accuracy is not significantly better than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not outperform a naive classifier that always predicts the majority class. The model predictions indicate 136 true negatives (non-severe flu cases correctly classified as âNoâ), and 11 true positives (severe flu cases correctly classified as âYesâ). The model gave 6 false positives (non-severe flu cases misclassified as âYesâ) and 47 false negatives (severe flu cases misclassified as âNoâ). Sensitivity was **0.1897**, which shows that the model correctly identified only 18.97% of severe flu cases. By comparison, specificity was **0.9577**, indicating that 95.77% of non-severe cases were correctly classified. The Kappa statistic was **0.1864**, indicating poor agreement between predicted and actual class labels, beyond what would be expected by chance. 

```{r}

# Set seed for reproducibility
set.seed(299)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

```
**Figure 6. ROC curve for the logistic regression model on the test dataset, showing an AUC of 0.703**

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.703**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds, although it is not extremely high performing.

## 4.4. Predictive Risk Score

Addressing the second study objective, predicted probabilities (predicted risk scores) were calculated based on the logistic regression model to quantify the likelihood of experiencing severe flu, based on individual participant characteristics.

```{r}

# Set seed for reproducibility
set.seed(299)

# Distribution of predicted risk scores from logistic regression model
risk_scores <- glm.pred

# predicted probabilities and severe flu outcomes
risk_df <- data.frame(
  risk_score = risk_scores,
  severe_flu = testing_data$severe_flu
)

# Plot distribution of risk scores
ggplot(risk_df, aes(x = risk_score, fill = severe_flu)) +
  geom_histogram(position = "identity", alpha = 0.6, bins = 30) +
  scale_fill_manual(values = c("lightblue", "pink")) +
  labs(x = "Predicted Probability (Risk Score)", 
       y = "Count",
       title = "Distribution of Severe Flu Risk Scores") 

```

**Figure 6. Distribution of severe flu risk scores by actual severe flu outcome. The histogram shows the distribution of predicted probabilities from the logistic regression model by outcome (blue = no severe flu, pink = severe flu).**

From the histogram plot, it is observed that there is substantial overlap between subjects who experienced severe flu and those who did not. Although patients who developed severe flu tend to be higher in the upper ranges of predicted probabilities (above 0.4) and slightly lower representation in the lowest risk range (0.1-0.2), many still received relatively low risk scores. The overlap in distributions for both severe and non-severe flu groups suggests the model performs relatively well in assigning higher risk to true cases, but it does not reliably distinguish the two groups. This likely reflects the model's modest discriminative ability as well as the class imbalance in the dataset.

The predicted risk of experiencing severe flu can be quantified using the fitted logistic regression model, which estimates the log-odds of severe flu as a linear combination of demographic and clinical predictors. The model equation is:

$$
\begin{aligned}
\log(\text{odds of severe flu}) =\ 
& -78.592 - 0.029 \cdot \text{age} + 0.310 \cdot \text{gender}_{\text{Male}} \\
& +\ 0.323 \cdot \text{race}_{\text{Asian}} - 0.176 \cdot \text{race}_{\text{Black}} + 0.417 \cdot \text{race}_{\text{Hispanic}} \\
& -\ 0.205 \cdot \text{smoking}_{\text{Former}} + 0.611 \cdot \text{smoking}_{\text{Current}} \\
& +\ 0.424 \cdot \text{height} - 0.451 \cdot \text{weight} + 1.544 \cdot \text{BMI} \\
& +\ 0.484 \cdot \text{diabetes}_{\text{Yes}} + 0.447 \cdot \text{hypertension}_{\text{Yes}} \\
& -\ 0.0117 \cdot \text{SBP} + 0.0118 \cdot \text{LDL}
\end{aligned}
$$
Note that the predicted probability for severe flu based on individual participant characteristics can be calculated as:

$$
P(\text{Severe Flu} = 1 \mid X) = \frac{e^{\text{log-odds}}}{1 + e^{\text{log-odds}}}
$$

where log-odds is the linear predictor estimated by the logistic regression model above.


### 4.5. Key demographic and clinical predictors of Severe Flu 

```{r}

# Set seed for reproducibility
set.seed(299)

# Get coefficients from logistic regression model
coef_table <- cbind(
  Coefficient = coef(model.glm$finalModel),
  OddsRatio = exp(coef(model.glm$finalModel))
)

# Format and display the table
kable(coef_table, digits = 3)

```
**Table 3. Logistic Regression model coefficients and odds ratios for severe flu prediction**

```{r}

# Create a coefficient plot for visualization
coef_plot_data <- coef_df[-1, ] # Remove intercept for plotting
coef_plot_data$Predictor <- factor(coef_plot_data$Predictor)
coef_plot_data$Direction <- ifelse(coef_plot_data$Coefficient > 0, "Positive", "Negative")

ggplot(coef_plot_data, aes(x = reorder(Predictor, Coefficient), y = Coefficient, fill = Direction)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = c("Negative" = "lightblue", "Positive" = "pink")) +
  labs(x = "Predictor", y = "Coefficient (Log Odds)") 

```

**Figure 7. Magnitude and direction of logistic regression model coefficients on log-odds of experiencing severe flu**

# 5. Limitations

The main limitation of this study is the class imbalance, which most likely contributed to low sensitivity across models. Although SVM radial kernel performed well on AUC, they did not significantly outperform the no information rate when evaluated at a 0.5 threshold. Additionally, black-box models offer limited interpretability, which makes them less practical in clinical or decision-making contexts. 

# 6. Conclusion



# References

Christodoulou, E., Ma, J., Collins, G. S., Steyerberg, E. W., Verbakel, J. Y., & Van Calster, B. (2019). A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models. Journal of clinical epidemiology, 110, 12-22.

Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). Applied logistic regression (3rd ed.). John Wiley & Sons.

Kerr, K. F., Wang, Z., Janes, H., McClelland, R. L., Psaty, B. M., & Pepe, M. S. (2014). Net reclassification indices for evaluating risk prediction instruments: A critical review. Epidemiology, 25(1), 114â121. https://doi.org/10.1097/EDE.0000000000000018:contentReference[oaicite:3]{index=3}

Kuhn, M., & Johnson, K. (2013). Applied predictive modeling. Springer.