---
title: "P8106 Final: Working Document"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "05/05/2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(patchwork)
library(corrplot)
library(pROC)
library(MASS)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(jpeg)
library(visdat)
library(vip)

```

# Data Preprocessing

```{r}

# Set seed for reproducibility
set.seed(299)

# Load dataset
flu_df <- read_csv("severe_flu.csv")

# Look at variable types
glimpse(flu_df)

# Check for missing data visually 
vis_miss(flu_df)

# Convert categorical variables to factors
flu_df <- flu_df %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")),
    race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, levels = c(0, 1), labels = c("No", "Yes")),
    hypertension = factor(hypertension, levels = c(0, 1), labels = c("No", "Yes")),
    severe_flu = factor(severe_flu, levels = c(0, 1), labels = c("No", "Yes"))
  )

# Drop ID
flu_df <- dplyr::select(flu_df, -id)

# Check data structure
str(flu_df)

# Set 10-fold cross-validation
ctrl1 <- trainControl(method = "cv", number = 10)

# Split data into testing and training
data_split <- initial_split(flu_df, prop = 0.8)
training_data <- training(data_split)
testing_data <- testing(data_split)

# Check levels of response variable
levels(training_data$severe_flu)

```

The data was loaded and inspected. No missing data was identified. Subsequently, factor variables were recoded appropriately to preserve categories. Data was then split into training and testing datasets.


# Exploratory Data Analysis

```{r}

# Histograms for numeric predictors 
h1 <- ggplot(training_data, aes(x = age)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Age Distribution") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h2 <- ggplot(training_data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("BMI") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h3 <- ggplot(training_data, aes(x = SBP)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("Systolic BP") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h4 <- ggplot(training_data, aes(x = LDL)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("LDL Cholesterol") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h5 <- ggplot(training_data, aes(x = height)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Height") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h6 <- ggplot(training_data, aes(x = weight)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Weight") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

# Combine plots
# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-y-axis-labels
combined_histogram_numeric <- (h1 + h2) / (h3 + h4) / (h5 + h6)
wrap_elements(combined_histogram_numeric) +
  labs(tag = "Count") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )


# Boxplots of numeric predictors
# Ref code: https://patchwork.data-imaginist.com/reference/plot_annotation.html
theme_no_xlab <- theme_bw() + theme(axis.title.x = element_blank())
bp1 <- ggplot(training_data, aes(x = severe_flu, y = age)) + geom_boxplot() + theme_no_xlab
bp2 <- ggplot(training_data, aes(x = severe_flu, y = bmi)) + geom_boxplot() + theme_no_xlab
bp3 <- ggplot(training_data, aes(x = severe_flu, y = SBP)) + geom_boxplot() + theme_no_xlab
bp4 <- ggplot(training_data, aes(x = severe_flu, y = LDL)) + geom_boxplot() + theme_no_xlab
bp5 <- ggplot(training_data, aes(x = severe_flu, y = height)) + geom_boxplot() + theme_no_xlab
bp6 <- ggplot(training_data, aes(x = severe_flu, y = weight)) + geom_boxplot() + theme_no_xlab

# Combine all plots
boxplot_numeric <- (bp1 + bp2) / (bp3 + bp4) / (bp5 + bp6) +
  plot_annotation(title = "Distribution of Numeric Predictors by Severe Flu Status")

# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-x-axis-labels 
wrap_elements(boxplot_numeric) +
  labs(tag = "Severe Flu") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom"
  )

# Correlation plot for numeric variables
numeric_vars <- training_data %>% dplyr::select(age, height, weight, bmi, SBP, LDL)
corrplot(cor(numeric_vars), method = "circle") 


# Bar Plots of Categorical Predictors by Outcome
bar1 <- ggplot(training_data, aes(x = gender, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + labs(fill = "Severe Flu") + theme_bw()

bar2 <- ggplot(training_data, aes(x = race, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar3 <- ggplot(training_data, aes(x = smoking, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar4 <- ggplot(training_data, aes(x = diabetes, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar5 <- ggplot(training_data, aes(x = hypertension, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL) + theme_bw() + theme(legend.position = "none")

# Combine using patchwork
((bar1 + bar2) / (bar3 + bar4) / (bar5 + plot_spacer())) +
  plot_layout(guides = "collect") +
  plot_annotation(title = "Proportion of Severe Flu by Categorical Predictor")


```

The plots show that the numeric predictors, including age, BMI, SBP, LDL, height, and weight, are approximately normally distributed, with minimal right skew for LDL. Age, SBP, LDL, and height distributions appear similar between severe flu and non-severe flu groups, with considerable middle quartile overlap. BMI and weight show slightly higher median values in the severe flu group, however, the substantial overlap in their distributions suggest these differences may not be statistically significant.

Among categorical predictors, there appears to be differences in severe flu rates. Hispanic subjects show a higher proportion of severe flu cases compared to other racial groups. Diabetic patients also appear to have a slightly higher severe flu rate than non-diabetic individuals. Similarly current smokers have a slightly higher proportion of severe flu cases compared to never and former smokers. Gender and hypertension status show minimal differences in severe flu proportions across categories.


# Simple Models

## Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)

# Set up cross validation control
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Train logistic regression model with CV
model.glm <- train(severe_flu ~ ., 
                   data = training_data,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)

# Predict probabilities on test data
glm.pred <- predict(model.glm, 
                    newdata = testing_data, 
                    type = "prob")[, "Yes"]

# Convert probabilities to class predictions, using 0.5 cutoff
glm.class <- rep("No", length(glm.pred))
glm.class[glm.pred > 0.5] <- "Yes"

# Confusion matrix
confusionMatrix(
  data = factor(glm.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# Compute ROC 
roc.glm <- roc(testing_data$severe_flu, glm.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

```

I proceeded with using confusion matrix to evaluate how well the model classifies observations into severe and non-severe flu categories. I used a 0.5 cutoff, following the Bayes decision rule, which assigns an observation to the most likely class. In this case, the model predicts 'severe' when Pr(Y=1 | X) > 0.5, and 'not severe' otherwise.

Based on the confusion matrix, the accuracy of the logistic regression model is **0.735**, or 73.5%. The misclassification rate is
1 − Accuracy = **0.265**, meaning about 26.5% of test observations were incorrectly classified.

The P-Value [Acc > NIR] is **0.2434**, which indicates that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not outperform a naive classifier that always predicts the majority class.

The matrix shows 136 true negatives (non-severe flu cases correctly classified as “No”), and 11 true positives (severe flu cases correctly classified as “Yes”). There are 6 false positives (non-severe flu cases misclassified as “Yes”) and 47 false negatives (severe flu cases misclassified as “No”).

Sensitivity was **0.1897**, which shows that the model correctly identified only 18.97% of severe flu cases.
Specificity was **0.9577**, indicating that 95.77% of non-severe cases were correctly classified.

The Kappa statistic was **0.1864**, indicating poor agreement between predicted and actual class labels, beyond what would be expected by chance.

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.703**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds, although it is not extremely high performing.


## Penalized Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)

# Set preprocessing
preproc <- c("center", "scale", "zv")


# Define tuning grid for penalized logistic regression
#glmnGrid <- expand.grid(.alpha = seq(0, 0.2, length = 21),
#                        .lambda = exp(seq(-8, 0, length = 50)))
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-8, 0, length = 50)))
# Fit model with CV
model.glmn <- train(severe_flu ~ .,
                    data = training_data,
                    preProcess = preproc,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

# Show best tuning parameters
model.glmn$bestTune

# Plot the results
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))


# Coefficients in the final model
coef(model.glmn$finalModel, model.glmn$bestTune$lambda)

```

I tuned elastic net logistic regression with cross-validation to predict severe flu occurrence. A preprocessing step was undertaken during model training to ensure numeric predictors were centered and scaled. Initially, I performed a grid search across alpha values from 0 to 1 and lambda values from exp(-8) to exp(0). When results showed optimal alpha=0, I refined the search with a narrower grid from 0 to 0.2 to confirm this finding. The optimal parameters (**alpha = 0, lambda = 0.8493658**) indicate that pure ridge regression provides the best predictive performance for this dataset. 

The plot displays Area Under the Curve on the y-axis, representing classification performance, against log-transformed lambda values (regularization) on the x-axis, with each line representing a different alpha mixing percentage. This confirms that ridge regression (alpha = 0) gives the largest AUC across regularization strengths, with the best performance observed at a lambda value of approximately 0.85.

The final model retained all predictors, although many were shrunk towards zero. The largest coefficients were observed for BMI (0.1089), weight (0.0674), and LDL (0.0333). Several other coefficients, including raceAsian and SBP, were shrunk to be closer to zero.

## LDA

```{r}

# Set seed for reproducibility
set.seed(299)


# Fit the LDA model using MASS package
lda.fit <- lda(severe_flu ~ ., data = training_data)

# Plot the histogram of discriminant scores (Z-variable, Z = a^T*X)
plot(lda.fit)

# Fit LDA model using caret
model.lda <- train(severe_flu ~ .,
                   data = training_data,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)

# Predict probabilities for test data
lda.pred <- predict(model.lda, newdata = testing_data, type = "prob")

# Classify using 0.5 cutoff
lda.class <- rep("No", length(lda.pred[, "Yes"]))
lda.class[lda.pred[, "Yes"] > 0.5] <- "Yes"

# Generate confusion matrix
confusionMatrix(
  data = factor(lda.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# ROC
roc.lda <- roc(testing_data$severe_flu, lda.pred[, "Yes"])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)

```

I fit the LDA model using the MASS package initially, examining the distribution of the LDA discriminant scores (Z = aᵗX) for each class. From the histogram, we can see there is some separation between the linear discriminant scores for the “No” and “Yes” severe flu groups. This suggests that the model is able to distinguish between the two classes using a linear combination of the predictors. I then proceeded to fit the model using caret.

Based on the confusion matrix from test data predictions, the accuracy of the LDA model is **0.74**, or 74.0%. The misclassification rate is 1 − Accuracy = **0.26**, meaning about 26% of test observations were incorrectly classified.

The **P-Value [Acc > NIR] is 0.1965**, which indicates that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not significantly outperform a naive classifier that always predicts the majority class.

The matrix shows 136 true negatives (non-severe flu cases correctly classified as “No”) and 12 true positives (severe flu cases correctly classified as “Yes”). There are 6 false positives (non-severe flu cases misclassified as “Yes”) and 46 false negatives (severe flu cases misclassified as “No”).

Sensitivity was **0.2069**, indicating that the model correctly identified only 20.69% of severe flu cases.
Specificity was **0.9577**, meaning 95.77% of non-severe cases were correctly classified.

The Kappa statistic was **0.2068**, indicating weak agreement between the predicted and actual class labels, although slightly better than what would be expected by chance.


The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.709**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds, although its performance is limited.

# Moderately Complex models

## MARS 

```{r}

# Set seed for reproducibility
set.seed(299)

# Tuning grid for degree and number of terms (nprune)
mars_grid <- expand.grid(
  degree = 1:4, # degree of interactions
  nprune = 2:20 # no. of retained terms
)

# Fit the MARS model
model.mars <- train(severe_flu ~ .,
                    data = training_data,
                    method = "earth",
                    tuneGrid = mars_grid,
                    metric = "ROC",
                    trControl = ctrl)

# Plot CV results
ggplot(model.mars)

# Best parameters
model.mars$bestTune


```

I opted to set `degree` to 1:4, allowing me to represent an appropriate level of interactions without too much complexity. 
For the number of retained terms (`nprune`), I expanded the range to 2:20, which allowed the model to show a stable performance plateau. The MARS model achieved the highest ROC using degree = 1 (i.e., no interactions) and approximately 3 retained terms.

```{r}

# Set seed for reproducibility
set.seed(299)

# Coefficients for final model
coef(model.mars$finalModel)

# Partial dependence plot for 'bmi'
pdp::partial(model.mars, pred.var = c("bmi"), grid.resolution = 200) |> autoplot()

# Partial dependence plot for 'LDL'
pdp::partial(model.mars, pred.var = c("LDL"), grid.resolution = 200) |> autoplot()

```

The final model can be represented as follows:

$$
\begin{aligned} 
\log(\text{odds of severe flu})= -2.26535278 + 0.35193909 × h(bmi-26.6) + 0.01153187 × h(LDL-77)
\end{aligned}
$$
where $h(x) = \max(0,x)$ is the hinge function.

The partial dependence plot shows that higher BMI values (greater than 26.6) are associated with lower predicted probability of severe flu. While the coefficient for h(bmi-26.6) is positive in the log-odds scale, there appears to be  different pattern observed in the plot, likely due to an averaging effect across all observations in the data and the non-linear transformation between log-odds and probability scales (Molnar, 2022; Goldstein et al., 2015).
For LDL, the partial dependence plot shows that after approximately 77 higher LDL values are associated with a decreasing predicted probability of severe flu. Similar to the BMI partial dependence plot, this observation appears to contradict the positive coefficient for h(LDL-77) in the final MARS model equation. Overall, this suggests the effects of predictors across the subject population in the data may merit careful consideration.

```{r}

# Variable importance plot for MARS
vip(model.mars$finalModel, type = "nsubsets")

```

The variable importance plot shows that BMI and LDL are the most influential predictors of severe flu in the final MARS model, with BMI being the most important predictor identified. The other predictors were not identified as being influential for the model.

## GAM

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit GAM model using caret
model.gam <- train(
  severe_flu ~ .,
  data = training_data,
  method = "gam",
  metric = "ROC",
  trControl = ctrl
)

# Summary
summary(model.gam)

# Examine final model parameters
model.gam$finalModel

# Plot smooth GAM terms
# Code source: https://bookdown.org/ndphillips/YaRrr/arranging-plots-with-parmfrow-and-layout.html
par(mfrow = c(3, 2))  
plot(model.gam$finalModel)
par(mfrow = c(1, 1)) # Reset plotting window

```

Among the continuous predictors, BMI and height are modelled as approximately linear with estimated degrees of freedom (edf) approximately 1 and statistically significant smooth terms (p < 0.05). Based on the predictor plots, these variables show linear relationships with severe flu, consistent with their estimated degrees of freedom. The smooth terms for LDL and weight are statistically significant (p < 0.05), with estimated degrees of freedom of 2.15 and 1.96 respectively, indicating nonlinear relationships with severe flu. The LDL plot appears to show flatness, indicating a slight nonlinear effect. In contrast, a clear  downward trend for weight is observable, showing slight nonlinearity represented by the smooth term.
Predictors age and SBP are not statistically significant, although they are modeled as approximately linear (edf = 1). 

# Complex Models

## SVM

```{r}

```


## Boosting

```{r}

```

# Model Comparison

```{r}

# Use cross validation to perform model selection
res <- resamples(list(Logistic_Regression = model.glm,
                      Penalised_Logistic_Regression = model.glmn,
                      LDA = model.lda,
                      MARS = model.mars,
                      GAM = model.gam))
summary(res)

```

# References

Molnar, C. (2022). Interpretable machine learning: A guide for making black box models explainable (2nd ed.). https://christophm.github.io/interpretable-ml-book

Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. journal of Computational and Graphical Statistics, 24(1), 44-65.

