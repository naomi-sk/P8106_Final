---
title: "P8106 Final: Working Document"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "05/05/2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(patchwork)
library(corrplot)
library(table1)
library(pROC)
library(MASS)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(jpeg)
library(visdat)
library(vip)
library(tidymodels)
library(knitr)

```

# Data Preprocessing

```{r}

# Set seed for reproducibility
set.seed(299)

# Load dataset
flu_df <- read_csv("severe_flu.csv")

# Look at variable types
glimpse(flu_df)

# Check for missing data visually 
vis_miss(flu_df)

# Convert categorical variables to factors
flu_df <- flu_df %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")),
    race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, levels = c(0, 1), labels = c("No", "Yes")),
    hypertension = factor(hypertension, levels = c(0, 1), labels = c("No", "Yes")),
    severe_flu = factor(severe_flu, levels = c(0, 1), labels = c("No", "Yes"))
  )

# Drop ID
flu_df <- dplyr::select(flu_df, -id)

# Check data structure
str(flu_df)

# Set 10-fold cross-validation
ctrl1 <- trainControl(method = "cv", number = 10)

# Split data into testing and training
data_split <- initial_split(flu_df, prop = 0.8)
training_data <- training(data_split)
testing_data <- testing(data_split)

# Check levels of response variable
levels(training_data$severe_flu)

```

The data was loaded and inspected. No missing data was identified. Subsequently, factor variables were recoded appropriately to preserve categories. Data was then split into training and testing datasets. The ID variable was removed.


# Exploratory Data Analysis

## Descriptive summary of Data

```{r}

# Set seed for reproducibility
set.seed(299)

# Reference code: https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html 

label(flu_df$age) <- "Age (yrs)"
label(flu_df$gender) <- "Gender"
label(flu_df$race) <- "Race/Ethnicity"
label(flu_df$smoking) <- "Smoking Status"
label(flu_df$height) <- "Height (cm)"
label(flu_df$weight) <- "Weight (kg)"
label(flu_df$bmi) <- "Body Mass Index (weight/ (height)^2)"
label(flu_df$diabetes) <- "Diabetes"
label(flu_df$hypertension) <- "Hypertension"
label(flu_df$SBP) <- "Systolic Blood Pressure (mmHg)"
label(flu_df$LDL) <- "LDL cholesterol (mg/dL)"

table1= table1(~ gender + race + smoking + height + weight + bmi + diabetes + hypertension + SBP + LDL, data=flu_df)
knitr::kable(table1)

```

## Summary of Predictors

```{r}

# Histograms for numeric predictors 
h1 <- ggplot(training_data, aes(x = age)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Age Distribution") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h2 <- ggplot(training_data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("BMI") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h3 <- ggplot(training_data, aes(x = SBP)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("Systolic BP") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h4 <- ggplot(training_data, aes(x = LDL)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("LDL Cholesterol") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h5 <- ggplot(training_data, aes(x = height)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Height") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h6 <- ggplot(training_data, aes(x = weight)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Weight") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

# Combine plots
# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-y-axis-labels
combined_histogram_numeric <- (h1 + h2) / (h3 + h4) / (h5 + h6)
wrap_elements(combined_histogram_numeric) +
  labs(tag = "Count") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )


# Boxplots of numeric predictors
# Ref code: https://patchwork.data-imaginist.com/reference/plot_annotation.html
theme_no_xlab <- theme_bw() + theme(axis.title.x = element_blank())
bp1 <- ggplot(training_data, aes(x = severe_flu, y = age)) + geom_boxplot() + theme_no_xlab
bp2 <- ggplot(training_data, aes(x = severe_flu, y = bmi)) + geom_boxplot() + theme_no_xlab
bp3 <- ggplot(training_data, aes(x = severe_flu, y = SBP)) + geom_boxplot() + theme_no_xlab
bp4 <- ggplot(training_data, aes(x = severe_flu, y = LDL)) + geom_boxplot() + theme_no_xlab
bp5 <- ggplot(training_data, aes(x = severe_flu, y = height)) + geom_boxplot() + theme_no_xlab
bp6 <- ggplot(training_data, aes(x = severe_flu, y = weight)) + geom_boxplot() + theme_no_xlab

# Combine all plots
boxplot_numeric <- (bp1 + bp2) / (bp3 + bp4) / (bp5 + bp6) +
  plot_annotation(title = "Distribution of Numeric Predictors by Severe Flu Status")

# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-x-axis-labels 
wrap_elements(boxplot_numeric) +
  labs(tag = "Severe Flu") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom"
  )

# Correlation plot for numeric variables
numeric_vars <- training_data %>% dplyr::select(age, height, weight, bmi, SBP, LDL)
corrplot(cor(numeric_vars), method = "circle") 


# Bar Plots of Categorical Predictors by Outcome
bar1 <- ggplot(training_data, aes(x = gender, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + labs(fill = "Severe Flu") + theme_bw()

bar2 <- ggplot(training_data, aes(x = race, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar3 <- ggplot(training_data, aes(x = smoking, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar4 <- ggplot(training_data, aes(x = diabetes, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar5 <- ggplot(training_data, aes(x = hypertension, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL) + theme_bw() + theme(legend.position = "none")

# Combine using patchwork
((bar1 + bar2) / (bar3 + bar4) / (bar5 + plot_spacer())) +
  plot_layout(guides = "collect") +
  plot_annotation(title = "Proportion of Severe Flu by Categorical Predictor")


```

The plots show that the numeric predictors, including age, BMI, SBP, LDL, height, and weight, are approximately normally distributed, with minimal right skew for LDL. Age, SBP, LDL, and height distributions appear similar between severe flu and non-severe flu groups, with considerable middle quartile overlap. BMI and weight show slightly higher median values in the severe flu group, however, the substantial overlap in their distributions suggest these differences may not be statistically significant.

The correlation plot shows several relationships among continuous predictors. Weight and BMI have a strong positive correlation (r ≈ 0.8), which may indicate colinearity. This is expected as BMI is calculated directly from weight and height. Therefore, interpretation of model results, including identifying important predictors of severe flu, should consider that these variables overlap  in what they measure.
Age and SBP have a moderate positive correlation (r ≈ 0.4). Height and weight are weakly positively correlated (r ≈ 0.2). By comparison, height has a moderate negative correlation with BMI (r ≈ -0.4). 

Among categorical predictors, there appears to be differences in severe flu rates. Hispanic subjects show a higher proportion of severe flu cases compared to other racial groups. Diabetic patients also appear to have a slightly higher severe flu rate than non-diabetic individuals. Similarly current smokers have a slightly higher proportion of severe flu cases compared to never and former smokers. Gender and hypertension status show minimal differences in severe flu proportions across categories.


# Simple Models

## Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)

# Set up cross validation control
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

# Train logistic regression model with CV
model.glm <- train(severe_flu ~ ., 
                   data = training_data,
                   method = "glm",
                   metric = "ROC",
                   trControl = ctrl)

# Predict probabilities on test data
glm.pred <- predict(model.glm, 
                    newdata = testing_data, 
                    type = "prob")[, "Yes"]

# Convert probabilities to class predictions, using 0.5 cutoff
glm.class <- rep("No", length(glm.pred))
glm.class[glm.pred > 0.5] <- "Yes"

# Confusion matrix
confusionMatrix(
  data = factor(glm.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# Compute ROC 
roc.glm <- roc(testing_data$severe_flu, glm.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

```

I proceeded with using confusion matrix to evaluate how well the model classifies observations into severe and non-severe flu categories. I used a 0.5 cutoff, following the Bayes decision rule, which assigns an observation to the most likely class. In this case, the model predicts 'severe' when Pr(Y=1 | X) > 0.5, and 'not severe' otherwise.

Based on the confusion matrix, the accuracy of the logistic regression model is **0.735**, or 73.5%. The misclassification rate is
1 − Accuracy = **0.265**, meaning about 26.5% of test observations were incorrectly classified.

The P-Value [Acc > NIR] is **0.2434**, which indicates that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not outperform a naive classifier that always predicts the majority class.

The matrix shows 136 true negatives (non-severe flu cases correctly classified as “No”), and 11 true positives (severe flu cases correctly classified as “Yes”). There are 6 false positives (non-severe flu cases misclassified as “Yes”) and 47 false negatives (severe flu cases misclassified as “No”).

Sensitivity was **0.1897**, which shows that the model correctly identified only 18.97% of severe flu cases.
Specificity was **0.9577**, indicating that 95.77% of non-severe cases were correctly classified.

The Kappa statistic was **0.1864**, indicating poor agreement between predicted and actual class labels, beyond what would be expected by chance.

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.703**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds, although it is not extremely high performing.


## Penalized Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)

# Set preprocessing
preproc <- c("center", "scale", "zv")


# Define tuning grid for penalized logistic regression
#glmnGrid <- expand.grid(.alpha = seq(0, 0.2, length = 21),
#                        .lambda = exp(seq(-8, 0, length = 50)))
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-8, 0, length = 50)))
# Fit model with CV
model.glmn <- train(severe_flu ~ .,
                    data = training_data,
                    preProcess = preproc,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    metric = "ROC",
                    trControl = ctrl)

# Show best tuning parameters
model.glmn$bestTune

# Plot the results
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))


# Coefficients in the final model
coef(model.glmn$finalModel, model.glmn$bestTune$lambda)

```

I tuned elastic net logistic regression with cross-validation to predict severe flu occurrence. A preprocessing step was undertaken during model training to ensure numeric predictors were centered and scaled. Initially, I performed a grid search across alpha values from 0 to 1 and lambda values from exp(-8) to exp(0). When results showed optimal alpha=0, I refined the search with a narrower grid from 0 to 0.2 to confirm this finding. The optimal parameters (**alpha = 0, lambda = 0.8493658**) indicate that pure ridge regression provides the best predictive performance for this dataset. 

The plot displays Area Under the Curve on the y-axis, representing classification performance, against log-transformed lambda values (regularization) on the x-axis, with each line representing a different alpha mixing percentage. This confirms that ridge regression (alpha = 0) gives the largest AUC across regularization strengths, with the best performance observed at a lambda value of approximately 0.85.

The final model retained all predictors, although many were shrunk towards zero. The largest coefficients were observed for BMI (0.1089), weight (0.0674), and LDL (0.0333). Several other coefficients, including raceAsian and SBP, were shrunk to be closer to zero.

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict on test data
glmn.pred <- predict(model.glmn, newdata = testing_data, type = "prob")[,2]

# Compute ROC
roc.glmn <- roc(testing_data$severe_flu, glmn.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.glmn, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glmn), col = 4, add = TRUE)


```

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.707**. This indicates the model is able to classify severe and non-severe cases across classification thresholds.

```{r}

# Set seed for reproducibility
set.seed(299)

# Classify using 0.5 cutoff
glmn.class <- rep("No", length(glmn.pred))
glmn.class[glmn.pred > 0.5] <- "Yes"

# Generate confusion matrix
confusionMatrix(
  data = factor(glmn.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# ROC
roc.glmn.fit <- roc(testing_data$severe_flu, glmn.pred)
plot(roc.glmn.fit, legacy.axes = TRUE, print.auc = TRUE)


```

These results show that at the default 0.5 classification threshold, the model failed to predict any positive cases, which corresponds with 0 sensitivity. This underscores a potential limitation of using a fixed threshold when there is an imbalance in classes, even though the model’s AUC (0.707) indicates moderate overall performance in classifying severe and non-severe flu cases.

## LDA

```{r}

# Set seed for reproducibility
set.seed(299)


# Fit the LDA model using MASS package
lda.fit <- lda(severe_flu ~ ., data = training_data)

# Plot the histogram of discriminant scores (Z-variable, Z = a^T*X)
plot(lda.fit)

# Fit LDA model using caret
model.lda <- train(severe_flu ~ .,
                   data = training_data,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)

# Predict probabilities for test data
lda.pred <- predict(model.lda, newdata = testing_data, type = "prob")

# Classify using 0.5 cutoff
lda.class <- rep("No", length(lda.pred[, "Yes"]))
lda.class[lda.pred[, "Yes"] > 0.5] <- "Yes"

# Generate confusion matrix
confusionMatrix(
  data = factor(lda.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# ROC
roc.lda <- roc(testing_data$severe_flu, lda.pred[, "Yes"])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)

```

I fit the LDA model using the MASS package initially, examining the distribution of the LDA discriminant scores (Z = aᵗX) for each class. From the histogram, we can see there is some separation between the linear discriminant scores for the “No” and “Yes” severe flu groups. This suggests that the model is able to distinguish between the two classes using a linear combination of the predictors. I then proceeded to fit the model using caret.

Based on the confusion matrix from test data predictions, the accuracy of the LDA model is **0.74**, or 74.0%. The misclassification rate is 1 − Accuracy = **0.26**, meaning about 26% of test observations were incorrectly classified.

The **P-Value [Acc > NIR] is 0.1965**, which indicates that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not significantly outperform a naive classifier that always predicts the majority class.

The matrix shows 136 true negatives (non-severe flu cases correctly classified as “No”) and 12 true positives (severe flu cases correctly classified as “Yes”). There are 6 false positives (non-severe flu cases misclassified as “Yes”) and 46 false negatives (severe flu cases misclassified as “No”).

Sensitivity was **0.2069**, indicating that the model correctly identified only 20.69% of severe flu cases.
Specificity was **0.9577**, meaning 95.77% of non-severe cases were correctly classified.

The Kappa statistic was **0.2068**, indicating weak agreement between the predicted and actual class labels, although slightly better than what would be expected by chance.


The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.709**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds, although its performance is limited.

# Moderately Complex models

## MARS 

```{r}

# Set seed for reproducibility
set.seed(299)

# Tuning grid for degree and number of terms (nprune)
mars_grid <- expand.grid(
  degree = 1:4, # degree of interactions
  nprune = 2:20 # no. of retained terms
)

# Fit the MARS model
model.mars <- train(severe_flu ~ .,
                    data = training_data,
                    method = "earth",
                    tuneGrid = mars_grid,
                    metric = "ROC",
                    trControl = ctrl)

# Plot CV results
ggplot(model.mars)

# Best parameters
model.mars$bestTune


```

I opted to set `degree` to 1:4, allowing me to represent an appropriate level of interactions without too much complexity. 
For the number of retained terms (`nprune`), I expanded the range to 2:20, which allowed the model to show a stable performance plateau. The MARS model achieved the highest ROC using degree = 1 (i.e., no interactions) and approximately 3 retained terms.

```{r}

# Set seed for reproducibility
set.seed(299)

# Coefficients for final model
coef(model.mars$finalModel)

# Partial dependence plot for 'bmi'
pdp::partial(model.mars, pred.var = c("bmi"), grid.resolution = 200) |> autoplot()

# Partial dependence plot for 'LDL'
pdp::partial(model.mars, pred.var = c("LDL"), grid.resolution = 200) |> autoplot()

```

The final model can be represented as follows:

$$
\begin{aligned} 
\log(\text{odds of severe flu})= -2.26535278 + 0.35193909 × h(bmi-26.6) + 0.01153187 × h(LDL-77)
\end{aligned}
$$
where $h(x) = \max(0,x)$ is the hinge function.

The partial dependence plot shows that higher BMI values (greater than 26.6) are associated with lower predicted probability of severe flu. While the coefficient for h(bmi-26.6) is positive in the log-odds scale, there appears to be  different pattern observed in the plot, likely due to an averaging effect across all observations in the data and the non-linear transformation between log-odds and probability scales (Molnar, 2022; Goldstein et al., 2015).
For LDL, the partial dependence plot shows that after approximately 77 higher LDL values are associated with a decreasing predicted probability of severe flu. Similar to the BMI partial dependence plot, this observation appears to contradict the positive coefficient for h(LDL-77) in the final MARS model equation. Overall, this suggests the effects of predictors across the subject population in the data may merit careful consideration.

```{r}

# Variable importance plot for MARS
vip(model.mars$finalModel, type = "nsubsets")

```

The variable importance plot shows that BMI and LDL are the most influential predictors of severe flu in the final MARS model, with BMI being the most important predictor identified. The other predictors were not identified as being influential for the model.

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict model on test data
mars.pred <- predict(model.mars, newdata = testing_data, type = "prob")[,2]

# Compute ROC
roc.mars <- roc(testing_data$severe_flu, mars.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.mars, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.mars), col = 4, add = TRUE)

```

Based on the ROC curve, the GAM model achieved an AUC of 0.668, representing modest classification performance on the test set. It is able to classify severe and non-severe flu cases better than chance, but it is not high performing.

## GAM

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit GAM model using caret
model.gam <- train(
  severe_flu ~ .,
  data = training_data,
  method = "gam",
  metric = "ROC",
  trControl = ctrl
)

# Summary
summary(model.gam)

# Examine final model parameters
model.gam$finalModel

# Plot smooth GAM terms
# Code source: https://bookdown.org/ndphillips/YaRrr/arranging-plots-with-parmfrow-and-layout.html
par(mfrow = c(3, 2))  
plot(model.gam$finalModel)
par(mfrow = c(1, 1)) # Reset plotting window

```

Among the continuous predictors, BMI and height are modelled as approximately linear with estimated degrees of freedom (edf) approximately 1 and statistically significant smooth terms (p < 0.05). Based on the predictor plots, these variables show linear relationships with severe flu, consistent with their estimated degrees of freedom. The smooth terms for LDL and weight are statistically significant (p < 0.05), with estimated degrees of freedom of 2.15 and 1.96 respectively, indicating nonlinear relationships with severe flu. The LDL plot appears to show flatness, indicating a slight nonlinear effect. In contrast, a clear  downward trend for weight is observable, showing slight nonlinearity represented by the smooth term.
Predictors age and SBP are not statistically significant, although they are modeled as approximately linear (edf = 1). 

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict model on test data
gam.pred <- predict(model.gam, newdata = testing_data, type = "prob")[,2]

# Compute ROC
roc.gam <- roc(testing_data$severe_flu, gam.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.gam, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.gam), col = 4, add = TRUE)


```

Based on the ROC curve, the GAM model achieved an AUC of 0.690, representing modest classification performance on the test set. It is able to classify severe and non-severe flu cases better than chance, but not with high reliability.

# Complex Models

## SVM

## Linear SVM

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model using SVM (linear)
svml.fit <- train(severe_flu ~ . ,
                  data = training_data,
                  method = "svmLinear",
                  tuneGrid = data.frame(C = exp(seq(-5, 2, len = 50))),
                  trControl = ctrl)

# Plot
plot(svml.fit, highlight = TRUE, xTrans = log)

# Find optimal
svml.fit$bestTune # optimal C = 0.05743262

exp(-5) # 0.006737947
exp(2) # 7.389056

```

The optimal cost tuning parameter is 0.057, selected from the grid exp(seq(-5, 2)). Since this value is not at the boundary, no further grid expansion is needed. 
Based on the cross-validation plot, the best performance was achieved at a cost value of approximately 0.057, with a maximum AUC of about 0.689. Note that the x-axis representing the cost parameter is log-scaled.

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict based on test data
svm.linear.pred <- predict(svml.fit, newdata = testing_data, type = "prob")[, "Yes"]
svm.linear.class <- ifelse(svm.linear.pred > 0.5, "Yes", "No")

# Confusion Matrix
confusionMatrix(factor(svm.linear.class, levels = c("No", "Yes")),
                testing_data$severe_flu,
                positive = "Yes")

# Plot AUC
roc.svm.linear <- roc(testing_data$severe_flu, svm.linear.pred)
plot(roc.svm.linear, legacy.axes = TRUE, print.auc = TRUE)

```

Based on the confusion matrix from test data predictions, the accuracy of the linear SVM model is **0.72**, or 72%. The misclassification rate is 1 − Accuracy = **0.28**, meaning 28% of test observations were incorrectly classified.

The **P-Value [Acc > NIR] is 0.4117**, indicating that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71). This would indicate that the model does not outperform a naive classifier that always predicts the majority class.

The confusion matrix shows 140 true negatives (non-severe flu cases correctly classified as “No”) and 4 true positives (severe flu cases correctly classified as “Yes”). There were 54 false negatives (severe flu cases misclassified as “No”) and 2 false positives (non-severe cases misclassified as “Yes”).

Sensitivity was **0.06897**, meaning 6.9% of severe flu cases were correctly identified. Specificity was comparably high at **0.98592**, indicating the model correctly identified 98.6% of non-severe cases.

The Kappa statistic was **0.0747**, showing very weak agreement between predicted and actual class labels, only slightly better than chance.

The ROC curve shows an **AUC of 0.699**, suggesting modest overall classification performance. While the model achieves a reasonable tradeoff between sensitivity and specificity across thresholds, it has very low sensitivity which suggests it may not be practically useful for classification.


## Radial Kernel SVM

```{r}

# Set seed for reproducibility
set.seed(299)

# Define tuning grid for Radial Kernel
svmr.grid <- expand.grid(C = exp(seq(-1, 5, len = 10)),
                         sigma = exp(seq(-10, -4, len = 6)))

# Fit radial kernel model
svmr.fit <- train(severe_flu ~ . , 
                  data = training_data,
                  method = "svmRadialSigma",
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

# Best sigma and cost
svmr.fit$bestTune

# Define colour scheme
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

# Plot
plot(svmr.fit, highlight = TRUE, par.settings = myPar)

```

I then proceeded to fit a support vector machine with radial kernel model, exploring different grids for the tuning parameters cost and sigma to ensure the optimal tuning parameters were appropriately within their respective grid ranges and not at the boundary of the grid. The best **cost** tuning parameter for this model is **2.718282**, and the best **sigma** tuning parameter is **0.0005004514**. 

The plot of cross-validated AUC shows the best performance was around 0.70 corresponding to the optimal tuning parameters identified for the radial support vector machine.

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict probabilities on test data using radial SVM
svm.radial.pred <- predict(svmr.fit, newdata = testing_data, type = "prob")[, "Yes"]

# Convert to class predictions using 0.5 cutoff
svm.radial.class <- ifelse(svm.radial.pred > 0.5, "Yes", "No")

# Confusion Matrix
confusionMatrix(factor(svm.radial.class, levels = c("No", "Yes")),
                testing_data$severe_flu,
                positive = "Yes")

# ROC and AUC
roc.svm.radial <- roc(testing_data$severe_flu, svm.radial.pred)
plot(roc.svm.radial, legacy.axes = TRUE, print.auc = TRUE)

```

Based on the confusion matrix from test data predictions, the accuracy of the radial kernel SVM model is **0.745**, or 74.5%. The misclassification rate is 1 − Accuracy = **0.255**, meaning 25.5% of test observations were incorrectly classified.

The **P-Value [Acc > NIR] is 0.1554**, indicating that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71). This would indicate that the model does not outperform a naive classifier that always predicts the majority class.

The confusion matrix shows 139 true negatives (non-severe flu cases correctly classified as “No”) and 10 true positives (severe flu cases correctly classified as “Yes”). There were 48 false negatives (severe flu cases misclassified as “No”) and 3 false positives (non-severe cases misclassified as “Yes”).

Sensitivity was **0.1724**, meaning 17.24% of severe flu cases were correctly identified. Specificity was higher at **0.9789**, indicating the model correctly identified 97.89% of non-severe cases.

The Kappa statistic was **0.1963**, representing weak agreement between predicted and actual class labels, only slightly better than chance.

The ROC curve shows an **AUC of 0.699**, indicating modest overall classification performance. The model achieves a reasonable tradeoff between sensitivity and specificity across thresholds, although its sensitivity is relatively low, limiting its practical use.

```{r}

# Set seed for reproducibility
set.seed(299)

```



## Boosting

```{r}

# Set seed for reproducibility
set.seed(299)

# Define tuning grid for parameters
gbmA.grid <- expand.grid(n.trees = c(100, 500, 1000, 5000), # no. of trees reduced for computational efficiency
                         interaction.depth = 1:3, # want to learn slowly, so keep small
                         shrinkage = c(0.001, 0.003, 0.005), # range of lambda values
                         n.minobsinnode = 10)


gbmA.fit <- train(severe_flu ~ . ,
                  training_data,
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

# Identify best tuning parameters
gbmA.fit$bestTune

# Plot to show best tuning parameters
ggplot(gbmA.fit, highlight = TRUE)

```

I then proceeded to perform boosting using adaboost. I tuned the grid keeping interaction.depth at 3, which was appropriate as it gave me n.trees and shrinkage values not at the boundary of their respective grids. Based on this model, the optimal tuning parameters were **n.trees = 500, interaction.depth = 1, shrinkage = 0.003, and n.minobsinnode = 10**, based on cross validation AUC. Based on the plot, the AUC appears to peak at approximately 0.69 for these parameters.

```{r}

# Set seed for reproducibility
set.seed(299)

# Presenting variable importance
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)

```

From this, we can see the most important predictor appears to be **BMI**, followed by **LDL** and **height**, respectively. The least important variables based on this model appear to be **HypertensionYes**, as well as **smokingFormer** and **raceBlack**.

Next, finding test error:

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict probabilities for positive class (ie Yes)
gbmA.prob <- predict(gbmA.fit, newdata = testing_data, type = "prob")[, "Yes"]

# Compute and plot ROC
roc.gbmA <- roc(testing_data$severe_flu, gbmA.prob)
plot(roc.gbmA, col = 2)

# Test AUC
auc_gbmA <- roc.gbmA$auc[1]
modelNames <- "Adaboost"
legend("bottomright", legend = paste0(modelNames, ": ", round(auc_gbmA,3)),
col = 1:2, lwd = 2)

```

To find the test performance for this model, I computed the AUC (area under the ROC curve). Based on the plot, **AUC was 0.674**, which represents moderate classification performance for predicting severe flu on the test set. This would indicate that the adaboost model has limited reliability in classifying severe flu cases.

# Model Comparison

```{r}

# Set seed for reproducibility
set.seed(299)

# Use cross validation to perform model selection
res <- resamples(list(Logistic_Regression = model.glm,
                      Penalised_Logistic_Regression = model.glmn,
                      LDA = model.lda,
                      MARS = model.mars,
                      GAM = model.gam,
                      SVM_Linear = svml.fit,
                      SVM_Radial_Kernel = svmr.fit,
                      AdaBoost = gbmA.fit))
summary(res)

# Extract mean AUC values 
roc_df <- as_tibble(summary(res)$statistics$ROC, rownames = "Model")

# Select only Model and AUC
roc_clean <- roc_df %>%
  transmute(Model, AUC = round(Mean, 3))

# Display table
kable(roc_clean)

```

The cross validation summary show that SVM with radial kernel achieved the highest mean AUC (0.701), followed closely by penalized logistic regression (0.696) and logistic regression (0.695).

Looking at test errors for the models:

```{r}

# Set seed for reproducibility
set.seed(299)

# Generate test set predictions for all models
glm.pred   <- predict(model.glm, newdata = testing_data, type = "prob")[, "Yes"]
glmn.pred  <- predict(model.glmn, newdata = testing_data, type = "prob")[, "Yes"]
lda.pred   <- predict(model.lda, newdata = testing_data, type = "prob")[, "Yes"]
mars.pred  <- predict(model.mars, newdata = testing_data, type = "prob")[, "Yes"]
gam.pred   <- predict(model.gam, newdata = testing_data, type = "prob")[, "Yes"]
svml.pred  <- predict(svml.fit, newdata = testing_data, type = "prob")[, "Yes"]
svmr.pred  <- predict(svmr.fit, newdata = testing_data, type = "prob")[, "Yes"]
gbmA.pred  <- predict(gbmA.fit, newdata = testing_data, type = "prob")[, "Yes"]

# Compute ROC for each
roc.glm <- roc(testing_data$severe_flu, glm.pred)
roc.glmn <- roc(testing_data$severe_flu, glmn.pred)
roc.lda <- roc(testing_data$severe_flu, lda.pred)
roc.mars <- roc(testing_data$severe_flu, mars.pred)
roc.gam <- roc(testing_data$severe_flu, gam.pred)
roc.svml <- roc(testing_data$severe_flu, svml.pred)
roc.svmr <- roc(testing_data$severe_flu, svmr.pred)
roc.gbmA <- roc(testing_data$severe_flu, gbmA.pred)

# List of AUCs
auc <- c(
  roc.glm$auc[1],
  roc.glmn$auc[1],
  roc.lda$auc[1],
  roc.mars$auc[1],
  roc.gam$auc[1],
  roc.svml$auc[1],
  roc.svmr$auc[1],
  roc.gbmA$auc[1]
)

modelNames <- c(
  "Logistic", "Penalized Logistic", "LDA", "MARS",
  "GAM", "SVM Linear", "SVM Radial", "AdaBoost"
)

# Plot ROC curves
ggroc(list(
  roc.glm, roc.glmn, roc.lda, roc.mars,
  roc.gam, roc.svml, roc.svmr, roc.gbmA
), legacy.axes = TRUE) +
  scale_color_discrete(
    labels = paste0(modelNames, " (", round(auc, 3), ")"),
    name = "Models (AUC)"
  ) +
  geom_abline(intercept = 0, slope = 1, color = "grey") +
  ggtitle("ROC Curves for All Models (Test Data Performance)") 

```

# References

Molnar, C. (2022). Interpretable machine learning: A guide for making black box models explainable (2nd ed.). https://christophm.github.io/interpretable-ml-book

Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. journal of Computational and Graphical Statistics, 24(1), 44-65.

