---
title: "P8106 Final: Working Document"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "05/05/2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries

```{r libraries, message=FALSE, warning=FALSE}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(patchwork)
library(corrplot)
library(table1)
library(pROC)
library(MASS)
library(factoextra)
library(gridExtra)
library(corrplot)
library(RColorBrewer)
library(gplots)
library(jpeg)
library(visdat)
library(vip)
library(tidymodels)
library(knitr)
library(pdp)
library(e1071)

```

# Data Preprocessing

```{r}

# Set seed for reproducibility
set.seed(299)

# Load dataset
flu_df <- read_csv("severe_flu.csv")

# Look at variable types
glimpse(flu_df)

# Check for missing data visually 
vis_miss(flu_df)

# Convert categorical variables to factors
flu_df <- flu_df %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Female", "Male")),
    race = factor(race, levels = c(1, 2, 3, 4), labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(smoking, levels = c(0, 1, 2), labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, levels = c(0, 1), labels = c("No", "Yes")),
    hypertension = factor(hypertension, levels = c(0, 1), labels = c("No", "Yes")),
    severe_flu = factor(severe_flu, levels = c(0, 1), labels = c("No", "Yes"))
  )

# Drop ID
flu_df <- dplyr::select(flu_df, -id)

# Check data structure
str(flu_df)

# Split data into testing and training
data_split <- initial_split(flu_df, prop = 0.8)
training_data <- training(data_split)
testing_data <- testing(data_split)

# Check levels of response variable
levels(training_data$severe_flu)

# Set preprocessing
preproc <- c("center", "scale", "zv")

# Set up cross validation control - with 5 repeats
ctrl <- trainControl(method = "repeatedcv", 
                     repeats = 5,
                     summaryFunction = defaultSummary,
                     classProbs = TRUE)

```

The data was loaded and inspected. No missing data was identified. Subsequently, factor variables were recoded appropriately to preserve categories. Data was then split into training and testing datasets. The ID variable was removed. An additional preprocessing setting was set.


# Exploratory Data Analysis

## Descriptive summary of Data

```{r}

# Set seed for reproducibility
set.seed(299)

# Reference code: https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html 

label(flu_df$age) <- "Age (yrs)"
label(flu_df$gender) <- "Gender"
label(flu_df$race) <- "Race/Ethnicity"
label(flu_df$smoking) <- "Smoking Status"
label(flu_df$height) <- "Height (cm)"
label(flu_df$weight) <- "Weight (kg)"
label(flu_df$bmi) <- "Body Mass Index (weight/ (height)^2)"
label(flu_df$diabetes) <- "Diabetes"
label(flu_df$hypertension) <- "Hypertension"
label(flu_df$SBP) <- "Systolic Blood Pressure (mmHg)"
label(flu_df$LDL) <- "LDL cholesterol (mg/dL)"

table1= table1(~ gender + race + smoking + height + weight + bmi + diabetes + hypertension + SBP + LDL, data=flu_df)
knitr::kable(table1)

```

## Summary of Predictors

```{r}

# Histograms for numeric predictors 
h1 <- ggplot(training_data, aes(x = age)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Age Distribution") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h2 <- ggplot(training_data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("BMI") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h3 <- ggplot(training_data, aes(x = SBP)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("Systolic BP") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h4 <- ggplot(training_data, aes(x = LDL)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("LDL Cholesterol") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h5 <- ggplot(training_data, aes(x = height)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Height") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

h6 <- ggplot(training_data, aes(x = weight)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Weight") +
  ylab("") +
  xlab("") +
  theme(plot.title = element_text(hjust = 0.5))

# Combine plots
# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-y-axis-labels
combined_histogram_numeric <- (h1 + h2) / (h3 + h4) / (h5 + h6)
wrap_elements(combined_histogram_numeric) +
  labs(tag = "Count") +
  theme(
    plot.tag = element_text(size = rel(1), angle = 90),
    plot.tag.position = "left"
  )


# Boxplots of numeric predictors
# Ref code: https://patchwork.data-imaginist.com/reference/plot_annotation.html
theme_no_xlab <- theme_bw() + theme(axis.title.x = element_blank())
bp1 <- ggplot(training_data, aes(x = severe_flu, y = age)) + geom_boxplot() + theme_no_xlab
bp2 <- ggplot(training_data, aes(x = severe_flu, y = bmi)) + geom_boxplot() + theme_no_xlab
bp3 <- ggplot(training_data, aes(x = severe_flu, y = SBP)) + geom_boxplot() + theme_no_xlab
bp4 <- ggplot(training_data, aes(x = severe_flu, y = LDL)) + geom_boxplot() + theme_no_xlab
bp5 <- ggplot(training_data, aes(x = severe_flu, y = height)) + geom_boxplot() + theme_no_xlab
bp6 <- ggplot(training_data, aes(x = severe_flu, y = weight)) + geom_boxplot() + theme_no_xlab

# Combine all plots
boxplot_numeric <- (bp1 + bp2) / (bp3 + bp4) / (bp5 + bp6) +
  plot_annotation(title = "Distribution of Numeric Predictors by Severe Flu Status")

# Ref code: https://tidytales.ca/snippets/2022-12-22_patchwork-shared-axis-labels/#shared-x-axis-labels 
wrap_elements(boxplot_numeric) +
  labs(tag = "Severe Flu") +
  theme(
    plot.tag = element_text(size = rel(1)),
    plot.tag.position = "bottom"
  )

# Correlation plot for numeric variables
numeric_vars <- training_data %>% dplyr::select(age, height, weight, bmi, SBP, LDL)
corrplot(cor(numeric_vars), method = "circle") 


# Bar Plots of Categorical Predictors by Outcome
bar1 <- ggplot(training_data, aes(x = gender, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + labs(fill = "Severe Flu") + theme_bw()

bar2 <- ggplot(training_data, aes(x = race, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar3 <- ggplot(training_data, aes(x = smoking, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar4 <- ggplot(training_data, aes(x = diabetes, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL)  + theme_bw() + theme(legend.position = "none")

bar5 <- ggplot(training_data, aes(x = hypertension, fill = severe_flu)) +
  geom_bar(position = "fill") + labs(y = NULL) + theme_bw() + theme(legend.position = "none")

# Combine using patchwork
((bar1 + bar2) / (bar3 + bar4) / (bar5 + plot_spacer())) +
  plot_layout(guides = "collect") +
  plot_annotation(title = "Proportion of Severe Flu by Categorical Predictor")


```

The plots show that the numeric predictors, including age, BMI, SBP, LDL, height, and weight, are approximately normally distributed, with minimal right skew for LDL. Age, SBP, LDL, and height distributions appear similar between severe flu and non-severe flu groups, with considerable middle quartile overlap. BMI and weight show slightly higher median values in the severe flu group, however, the substantial overlap in their distributions suggest these differences may not be statistically significant.

The correlation plot shows several relationships among continuous predictors. Weight and BMI have a strong positive correlation (r ≈ 0.8), which may indicate colinearity. This is expected as BMI is calculated directly from weight and height. Therefore, interpretation of model results, including identifying important predictors of severe flu, should consider that these variables overlap  in what they measure.
Age and SBP have a moderate positive correlation (r ≈ 0.4). Height and weight are weakly positively correlated (r ≈ 0.2). By comparison, height has a moderate negative correlation with BMI (r ≈ -0.4). 

Among categorical predictors, there appears to be differences in severe flu rates. Hispanic subjects show a higher proportion of severe flu cases compared to other racial groups. Diabetic patients also appear to have a slightly higher severe flu rate than non-diabetic individuals. Similarly current smokers have a slightly higher proportion of severe flu cases compared to never and former smokers. Gender and hypertension status show minimal differences in severe flu proportions across categories.


# Simple Models

## Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)

# Train logistic regression model with CV
model.glm <- train(severe_flu ~ ., 
                   data = training_data,
                   method = "glm",
                   preProcess = preproc,
                   trControl = ctrl)

# Predict probabilities on test data
glm.pred <- predict(model.glm, 
                    newdata = testing_data, 
                    type = "prob")[, "Yes"]

# Convert probabilities to class predictions, using 0.3 cutoff
glm.class <- rep("No", length(glm.pred))
glm.class[glm.pred > 0.3] <- "Yes"

# Confusion matrix
confusionMatrix(
  data = factor(glm.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# Compute ROC 
roc.glm <- roc(testing_data$severe_flu, glm.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)

```

I proceeded with using confusion matrix to evaluate how well the model classifies observations into severe and non-severe flu categories. I used a 0.3 probability threshold to assign predicted classes. This threshold was selected in line with the recommendation by Saito and Rehmsmeier (2015), who note that threshold selection make careful consideration of imbalanced classification problems. Under this rule, the model classifies an observation as “severe flu” when the predicted probability Pr(Y=1∣X)>0.3, and “non-severe flu” otherwise. This adjustment was chosen to prioritise sensitivity by reducing the likelihood of false negatives, which is relevant for our study.

Based on the confusion matrix, the accuracy of the logistic regression model is **0.695**, or 69.5%. The misclassification rate is
1 − Accuracy = **0.305**, meaning about 30.5% of test observations were incorrectly classified.

The P-Value [Acc > NIR] is **0.7098**, which indicates that the model’s accuracy is not statistically significantly greater than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not outperform a naive classifier that always predicts the majority class.

The matrix shows 112 true negatives (non-severe flu cases correctly classified as “No”), and 27 true positives (severe flu cases correctly classified as “Yes”). There are 30 false positives (non-severe flu cases misclassified as “Yes”) and 31 false negatives (severe flu cases misclassified as “No”).

Sensitivity was **0.4655**, which shows that the model correctly identified 46.55% of severe flu cases.
Specificity was **0.7887**, indicating that 78.87% of non-severe cases were correctly classified.

The Kappa statistic was **0.2556**, indicating fair agreement between predicted and actual class labels, beyond what would be expected by chance.

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.703**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds.


## Penalized Logistic Regression

```{r}

# Set seed for reproducibility
set.seed(299)


# Define tuning grid for penalized logistic regression
glmnGrid <- expand.grid(.alpha = seq(0, 1, length = 21),
                        .lambda = exp(seq(-8, 0, length = 50)))

# Fit model with CV
model.glmn <- train(severe_flu ~ .,
                    data = training_data,
                    preProcess = preproc,
                    method = "glmnet",
                    tuneGrid = glmnGrid,
                    trControl = ctrl)

# Show best tuning parameters
model.glmn$bestTune

# Plot the results
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))
plot(model.glmn, par.settings = myPar, xTrans = function(x) log(x))


# Coefficients in the final model
coef(model.glmn$finalModel, model.glmn$bestTune$lambda)

```

I tuned elastic net logistic regression with cross-validation to predict severe flu occurrence. A preprocessing step was undertaken during model training to ensure numeric predictors were centered and scaled. I performed a grid search across alpha values from 0 to 1 and lambda values from exp(-8) to exp(0). The optimal parameters (**alpha = 0.85, lambda = 0.003883492**) were identified, favouring lasso.

The plot displays cross-validated accuracy on the y-axis, representing classification performance on the training data, against log-transformed lambda values (regularisation) on the x-axis, with each curve representing a different alpha mixing percentage. This confirms that lasso dominant model gives the best accuracy across regularization strengths, with the best performance observed at a lambda value of approximately 0.0039.

The final model retained all predictors except weight. although many were shrunk towards zero. The largest coefficients were observed for BMI (0.702), LDL (0.184), diabetesYes (0.150), and smokingCurrent (0.150), indicating these variables contributed most strongly to predicting severe flu in the model. Several other variables, including SBP (–0.0013), height (–0.0257), and raceAsian (0.0468) were shrunk closer to 0, and therefore had smaller effect.


```{r}

# Set seed for reproducibility
set.seed(299)

# Predict on test data
glmn.pred <- predict(model.glmn, newdata = testing_data, type = "prob")[,2]

# Compute ROC
roc.glmn <- roc(testing_data$severe_flu, glmn.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.glmn, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glmn), col = 4, add = TRUE)


```

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.710**. This indicates the model is able to classify severe and non-severe cases across classification thresholds.

```{r}

# Set seed for reproducibility
set.seed(299)

# Classify using 0.3 cutoff
glmn.class <- rep("No", length(glmn.pred))
glmn.class[glmn.pred > 0.3] <- "Yes"

# Generate confusion matrix
confusionMatrix(
  data = factor(glmn.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# ROC
roc.glmn.fit <- roc(testing_data$severe_flu, glmn.pred)
plot(roc.glmn.fit, legacy.axes = TRUE, print.auc = TRUE)


```

Based on the confusion matrix, the accuracy of the penalized logistic regression model is **0.695**, or 69.5%. The misclassification rate is
1 − Accuracy = **0.305**, meaning about 30.5% of test observations were incorrectly classified.

The P-Value [Acc > NIR] is **0.7098**, which indicates that the model’s accuracy is not statistically significantly greater than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not outperform a naive classifier that always predicts the majority class.

The matrix shows 109 true negatives (non-severe flu cases correctly classified as “No”) and 30 true positives (severe flu cases correctly classified as “Yes”). There are 33 false positives (non-severe flu cases misclassified as “Yes”) and 28 false negatives (severe flu cases misclassified as “No”).

Sensitivity was **0.5172**, which shows that the model correctly identified 51.72% of severe flu cases.
Specificity was **0.7676**, indicating that 76.76% of non-severe cases were correctly classified.

The Kappa statistic was **0.2778**, indicating fair agreement between predicted and actual class labels, beyond what would be expected by chance.

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.710**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds.

## LDA

```{r}

# Set seed for reproducibility
set.seed(299)


# Fit the LDA model using MASS package
lda.fit <- lda(severe_flu ~ ., data = training_data)

# Plot the histogram of discriminant scores (Z-variable, Z = a^T*X)
plot(lda.fit)

# Fit LDA model using caret
model.lda <- train(severe_flu ~ .,
                   data = training_data,
                   preProcess = preproc,
                   method = "lda",
                   trControl = ctrl)

# Predict probabilities for test data
lda.pred <- predict(model.lda, newdata = testing_data, type = "prob")

# Classify using 0.3 cutoff
lda.class <- rep("No", length(lda.pred[, "Yes"]))
lda.class[lda.pred[, "Yes"] > 0.3] <- "Yes"

# Generate confusion matrix
confusionMatrix(
  data = factor(lda.class, levels = c("No", "Yes")),
  reference = testing_data$severe_flu,
  positive = "Yes"
)

# ROC
roc.lda <- roc(testing_data$severe_flu, lda.pred[, "Yes"])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)

```

I fit the LDA model using the MASS package initially, examining the distribution of the LDA discriminant scores (Z = aᵗX) for each class. From the histogram, we can see there is very little separation between the linear discriminant scores for the “No” and “Yes” severe flu groups. I then proceeded to fit the model using caret.

Based on the confusion matrix from test data predictions, the accuracy of the LDA model is **0.715**, or 71.5%. The misclassification rate is 1 − Accuracy = **0.285**, meaning about 28.5% of test observations were incorrectly classified.

The **P-Value [Acc > NIR] is 0.4733**, which indicates that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71), representing the accuracy that would be achieved by always predicting the most frequent class. Therefore, this model does not significantly outperform a naive classifier that always predicts the majority class.

The matrix shows 116 true negatives (non-severe flu cases correctly classified as “No”) and 27 true positives (severe flu cases correctly classified as “Yes”). There are 26 false positives (non-severe flu cases misclassified as “Yes”) and 31 false negatives (severe flu cases misclassified as “No”).

Sensitivity was **0.4655**, indicating that the model correctly identified 46.55% of severe flu cases.
Specificity was **0.8169**, meaning 81.69% of non-severe cases were correctly classified.

The Kappa statistic was **0.2898**, indicating fair agreement between the predicted and actual class labels, slightly better than what would be expected by chance.

The ROC curve indicates the model has moderate classification performance, with an **AUC of 0.709**. This means that the model achieves a reasonable tradeoff between sensitivity and specificity across different classification thresholds.

# Moderately Complex models

## MARS 

```{r}

# Set seed for reproducibility
set.seed(299)

# Tuning grid for degree and number of terms (nprune)
mars_grid <- expand.grid(
  degree = 1:4, # degree of interactions
  nprune = 2:20 # no. of retained terms
)

# Fit the MARS model
model.mars <- train(severe_flu ~ .,
                    data = training_data,
                    method = "earth",
                    tuneGrid = mars_grid,
                    trControl = ctrl)

# Plot CV results
ggplot(model.mars)

# Best parameters
model.mars$bestTune


```

I opted to set `degree` to 1:4, allowing me to represent an appropriate level of interactions without too much complexity. 
For the number of retained terms (`nprune`), I expanded the range to 2:20, which allowed the model to show a stable performance plateau. The MARS model achieved the highest ROC using degree = 1 (i.e., no interactions) and approximately 3 retained terms.

```{r}

# Set seed for reproducibility
set.seed(299)

# Coefficients for final model
coef(model.mars$finalModel)

# Partial dependence plot for 'bmi'
pdp::partial(model.mars, pred.var = c("bmi"), grid.resolution = 200) |> autoplot()

# Partial dependence plot for 'LDL'
pdp::partial(model.mars, pred.var = c("LDL"), grid.resolution = 200) |> autoplot()

```

The final model can be represented as follows:

$$
\begin{aligned} 
\log(\text{odds of severe flu})= -2.26535278 + 0.35193909 × h(bmi-26.6) + 0.01153187 × h(LDL-77)
\end{aligned}
$$
where $h(x) = \max(0,x)$ is the hinge function.

The partial dependence plot shows that higher BMI values (greater than 26.6) are associated with lower predicted probability of severe flu. While the coefficient for h(bmi-26.6) is positive in the log-odds scale, there appears to be  different pattern observed in the plot, likely due to an averaging effect across all observations in the data and the non-linear transformation between log-odds and probability scales (Molnar, 2022; Goldstein et al., 2015).
For LDL, the partial dependence plot shows that after approximately 77 higher LDL values are associated with a decreasing predicted probability of severe flu. Similar to the BMI partial dependence plot, this observation appears to contradict the positive coefficient for h(LDL-77) in the final MARS model equation. Overall, this suggests the effects of predictors across the subject population in the data may merit careful consideration.

```{r}

# Variable importance plot for MARS
vip(model.mars$finalModel, type = "nsubsets")

```

The variable importance plot shows that BMI and LDL are the most influential predictors of severe flu in the final MARS model, with BMI being the most important predictor identified. The other predictors were not identified as being influential for the model.

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict model on test data
mars.pred <- predict(model.mars, newdata = testing_data, type = "prob")[,2]

# Compute ROC
roc.mars <- roc(testing_data$severe_flu, mars.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.mars, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.mars), col = 4, add = TRUE)

```

Based on the ROC curve, the MARS model achieved an AUC of 0.668, representing modest classification performance on the test set. It is able to classify severe and non-severe flu cases better than chance, but it is not extremely high performing.

## GAM

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit GAM model using caret
model.gam <- train(
  severe_flu ~ .,
  data = training_data,
  method = "gam",
  trControl = ctrl
)

# Summary
summary(model.gam)

# Examine final model parameters
model.gam$finalModel

# Plot smooth GAM terms
# Code source: https://bookdown.org/ndphillips/YaRrr/arranging-plots-with-parmfrow-and-layout.html
par(mfrow = c(3, 2))  
plot(model.gam$finalModel)
par(mfrow = c(1, 1)) # Reset plotting window

```

Among the continuous predictors, BMI and height are modelled as approximately linear with estimated degrees of freedom (edf) approximately 1 and statistically significant smooth terms (p < 0.05). Based on the predictor plots, these variables show linear relationships with severe flu, consistent with their estimated degrees of freedom. The smooth terms for LDL and weight are statistically significant (p < 0.05), with estimated degrees of freedom of 2.15 and 1.96 respectively, indicating nonlinear relationships with severe flu. The LDL plot appears to show some nonlinearity but is comparably flat. In contrast, a clear  downward trend for weight is observable, showing slight nonlinearity represented by the smooth term. Predictors age and SBP are not statistically significant, although they are modeled as approximately linear (edf = 1). 

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict model on test data
gam.pred <- predict(model.gam, newdata = testing_data, type = "prob")[,2]

# Compute ROC
roc.gam <- roc(testing_data$severe_flu, gam.pred)

# Plot the ROC curve and the smoothed ROC curve
plot(roc.gam, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.gam), col = 4, add = TRUE)


```

Based on the ROC curve, the GAM model achieved an AUC of 0.690, representing modest classification performance on the test set. It is able to classify severe and non-severe flu cases better than chance, but not with high reliability.

# Complex Models

## SVM

## Linear SVM

```{r}

# Set seed for reproducibility
set.seed(299)

# Fit model using SVM (linear)
svml.fit <- train(severe_flu ~ . ,
                  data = training_data,
                  preProcess = preproc,
                  method = "svmLinear",
                  tuneGrid = data.frame(C = exp(seq(-6, 2, len = 50))),
                  trControl = ctrl)

# Plot
plot(svml.fit, highlight = TRUE, xTrans = log)

# Find optimal
svml.fit$bestTune # optimal C = 0.004762708

```

The optimal cost tuning parameter is approximately 0.00476, selected from the grid exp(seq(-6, 2)). Since this value is not at the boundary, no further grid expansion is needed. 
Based on the cross-validation plot, the best performance was achieved at a cost value of approximately 0.00476, with a maximum accuracy of about 0.689. Note that the x-axis representing the cost parameter is log-scaled.

```{r}

# Set seed for reproducibility
set.seed(299)

# Confusion Matrix based on test data - for Accuracy/Kappa
confusionMatrix(
  factor(predict(svml.fit, newdata = testing_data), levels = c("No", "Yes")),
  testing_data$severe_flu,
  positive = "Yes"
)

```

Based on the confusion matrix from test data predictions, the accuracy of the linear SVM model is **0.735**, or 73.5%. The misclassification rate is 1 − Accuracy = **0.265**, meaning 26.5% of test observations were incorrectly classified.

The **P-Value [Acc > NIR] is 0.2434**, indicating that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71). This would indicate that the model does not outperform a naive classifier that always predicts the majority class.
The Kappa statistic was **0.1648**, showing poor agreement between predicted and actual class labels, only slightly better than chance.


## Radial Kernel SVM

```{r}

# Set seed for reproducibility
set.seed(299)

# Define tuning grid for Radial Kernel
svmr.grid <- expand.grid(C = exp(seq(-1, 5, len = 10)),
                         sigma = exp(seq(-12, -2, len = 10)))

# Fit radial kernel model
svmr.fit <- train(severe_flu ~ . , 
                  data = training_data,
                  method = "svmRadialSigma",
                  preProcess = preproc,
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

# Best sigma and cost
svmr.fit$bestTune # sigma = 0.00482795

# Plot
plot(svmr.fit, highlight = TRUE, xTrans = log)

```

I then proceeded to fit a support vector machine with radial kernel model, exploring different grids for the tuning parameters cost and sigma to ensure the optimal tuning parameters were appropriately within their respective grid ranges and not at the boundary of the grid. The best **cost** tuning parameter for this model is **39.12128**, and the best **sigma** tuning parameter is **0.00482795**. This is evident in the plot, which represents model accuracy across the full range of sigma values at different cost levels.


```{r}

# Set seed for reproducibility
set.seed(299)

# Confusion Matrix based on test data - for Accuracy/Kappa
confusionMatrix(
  factor(predict(svmr.fit, newdata = testing_data), levels = c("No", "Yes")),
  testing_data$severe_flu,
  positive = "Yes"
)

```

Based on the confusion matrix from test data predictions, the accuracy of the radial kernel SVM model is **0.745**, or 74.5%. The misclassification rate is 1 − Accuracy = **0.255**, meaning 25.5% of test observations were incorrectly classified.

The **P-Value [Acc > NIR] is 0.1554**, indicating that the model’s accuracy is not significantly better than the no information rate (NIR = 0.71). This would indicate that the model does not outperform a naive classifier that always predicts the majority class.

The Kappa statistic was **0.1963**, representing poor agreement between predicted and actual class labels, only slightly better than chance.


```{r}

# Set seed for reproducibility
set.seed(299)

# PDP for age
pdp_age <- partial(svmr.fit, pred.var = "age", train = training_data, prob = TRUE)
plot_age <- autoplot(pdp_age) + ggtitle("Age") + theme(plot.title = element_text(size = 10))

# PDP for height
pdp_height <- partial(svmr.fit, pred.var = "height", train = training_data, prob = TRUE)
plot_height <- autoplot(pdp_height) + ggtitle("Height") + theme(plot.title = element_text(size = 10))

# PDP for weight
pdp_weight <- partial(svmr.fit, pred.var = "weight", train = training_data, prob = TRUE)
plot_weight <- autoplot(pdp_weight) + ggtitle("Weight") + theme(plot.title = element_text(size = 10))

# PDP for bmi
pdp_bmi <- partial(svmr.fit, pred.var = "bmi", train = training_data, prob = TRUE)
plot_bmi <- autoplot(pdp_bmi) + ggtitle("BMI") + theme(plot.title = element_text(size = 10))

# PDP for SBP
pdp_sbp <- partial(svmr.fit, pred.var = "SBP", train = training_data, prob = TRUE)
plot_sbp <- autoplot(pdp_sbp) + ggtitle("SBP") + theme(plot.title = element_text(size = 10))

# PDP for LDL
pdp_ldl <- partial(svmr.fit, pred.var = "LDL", train = training_data, prob = TRUE)
plot_ldl <- autoplot(pdp_ldl) + ggtitle("LDL") + theme(plot.title = element_text(size = 10))

# Combine into one figure (2 rows of 3)
(plot_age | plot_height | plot_weight) / (plot_bmi | plot_sbp | plot_ldl)

```

**Figure. The partial dependence plots are calculated using the SVM's decision function outputs, as the model does not provide probability estimates.  The estimated risk of severe flu increased with weight and decreased at lower values, whereas it declined over higher BMI. Age and SBP showed nonlinear patterns, with the risk of being classified as having severe flu reaching a peak at the middle of the range. Higher LDL and height were associated with lower estimate risk of severe flu.**

As SVM with radial kernel is a black-box model, partial dependence plots for interpretation are needed.

```{r}

# Set seed for reproducibility
set.seed(299)

# Match caret tuning parameters: cost = 39.12128, sigma = 0.00482795
best_sigma <- 0.00482795
best_cost <- 39.12128
best_gamma <- 1 / (2 * best_sigma^2)

# Refit using e1071::svm
svm.radial.e1071 <- svm(severe_flu ~ .,
                        data = training_data,
                        kernel = "radial",
                        cost = best_cost,
                        gamma = best_gamma,
                        probability = TRUE)

# Plot decision boundary 
plot(svm.radial.e1071,
     data = training_data,
     age ~ LDL,
     slice = list(gender = "Male",
                  race = "White",
                  smoking = "Never",
                  height = 173.1,
                  weight = 91.8,
                  bmi = 29,
                  diabetes = "Yes",
                  hypertension = "No",
                  SBP = 116),
     grid = 100,
     symbolPalette = c("cyan", "darkblue"),
     color.palette = heat.colors)

```

**Figure. SVM radial kernel decision boundary using age and LDL as predictors. The overdominating red indicates significant class imbalance in the dataset, representing the model’s tendency to classify most observations as non-severe flu.**


## Boosting

```{r}

# Set seed for reproducibility
set.seed(299)

# Define tuning grid for parameters
gbmA.grid <- expand.grid(n.trees = c(100, 500, 1000, 5000), # no. of trees reduced for computational efficiency
                         interaction.depth = 1:3, # want to learn slowly, so keep small
                         shrinkage = c(0.001, 0.003, 0.005), # range of lambda values
                         n.minobsinnode = 10)


gbmA.fit <- train(severe_flu ~ . ,
                  training_data,
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  verbose = FALSE)

# Identify best tuning parameters
gbmA.fit$bestTune

# Plot to show best tuning parameters
ggplot(gbmA.fit, highlight = TRUE)

```

I then proceeded to perform boosting using adaboost. I tuned the grid keeping interaction.depth at 3, which was appropriate as it gave me n.trees and shrinkage values not at the boundary of their respective grids. Based on this model, the optimal tuning parameters were **n.trees = 500, interaction.depth = 2, shrinkage = 0.003, and n.minobsinnode = 10**, based on cross validation accuracy. Based on the plot, the accuracy appears to peak at approximately 0.79 for these parameters.

```{r}

# Set seed for reproducibility
set.seed(299)

# Presenting variable importance
summary(gbmA.fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)

```

From this, we can see the most important predictor appears to be **BMI**, followed by **LDL** and **age**, respectively. The least important variables based on this model appear to be **raceAsian**, as well as **raceBlack** and **hypertensionYes**.

Next, finding test error:

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict probabilities for positive class (ie Yes)
gbmA.prob <- predict(gbmA.fit, newdata = testing_data, type = "prob")[, "Yes"]

# Compute and plot ROC
roc.gbmA <- roc(testing_data$severe_flu, gbmA.prob)
plot(roc.gbmA, col = 2)

# Test AUC
auc_gbmA <- roc.gbmA$auc[1]
modelNames <- "Adaboost"
legend("bottomright", legend = paste0(modelNames, ": ", round(auc_gbmA,3)),
col = 1:2, lwd = 2)

```

To find the test performance for this model, I computed the AUC (area under the ROC curve). Based on the plot, **AUC was 0.665**, which represents moderate classification performance for predicting severe flu on the test set. 

# Model Comparison

```{r}

# Set seed for reproducibility
set.seed(299)

# Use cross validation to perform model selection
res <- resamples(list(Logistic_Regression = model.glm,
                      Penalised_Logistic_Regression = model.glmn,
                      LDA = model.lda,
                      MARS = model.mars,
                      GAM = model.gam,
                      SVM_Linear = svml.fit,
                      SVM_Radial_Kernel = svmr.fit,
                      AdaBoost = gbmA.fit))
summary(res)

bwplot(res)

```

* SVM Radial Kernel had the highest mean accuracy (0.7755392), however logistic regression had a very similar mean accuracy as well (0.7750330).
* LDA (0.22142223) and GAM (0.21998921) had the highest Kappa values. However, logistic regression was also very similar (0.21524449).
* Both SVM Linear and SVM Radial kernel had comparably lower Kappa values (0.09512335 and 0.17142539, respectively).
* Therefore, GAM, LDA, and Logistic Regression offer the best combination of accuracy and Kappa.

# References

McHugh, M. L. (2012). Interrater reliability: the kappa statistic. Biochemia medica, 22(3), 276-282.

Molnar, C. (2022). Interpretable machine learning: A guide for making black box models explainable (2nd ed.). https://christophm.github.io/interpretable-ml-book

Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. journal of Computational and Graphical Statistics, 24(1), 44-65.

Saito, T., & Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. PloS one, 10(3), e0118432.

